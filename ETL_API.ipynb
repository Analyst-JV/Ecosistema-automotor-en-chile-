{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e004a-a301-4075-a816-ee0c80fed71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f32c90-008a-407d-a1ad-b3cd76d66b9f",
   "metadata": {},
   "source": [
    "Preparamos la importacion de los codigos y nombres estandar utilizados desde 2018 para completar cuando nos falten datos y estandarizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a6109-e795-47c6-a8e4-ee5dff0fdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codigos = pd.read_excel(\"CUT_2018.xls\", engine=\"xlrd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f2ca6-34fe-4f5f-b5d8-38f9bdd25565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codigos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d582637-0c83-44e8-8467-3311f9793554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codigos = df_codigos.astype({\n",
    "    \"Código Región\":       \"Int64\",\n",
    "    \"Nombre Región\":       \"string\",\n",
    "    \"Abreviatura Región\":  \"string\",\n",
    "    \"Código Provincia\":    \"Int64\",\n",
    "    \"Nombre Provincia\":    \"string\",\n",
    "    \"Código Comuna 2018\":  \"Int64\",\n",
    "    \"Nombre Comuna\":       \"string\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c5dfc-013f-46be-bb59-020f3766df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codigos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce793577-5858-4a56-9f0e-fa5fd38e9cd7",
   "metadata": {},
   "source": [
    "Descargar las bases segun cada año utilizando el inventario de nombres descubiertos en el analisis exploratorio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92b608-cad3-4572-86af-1a49761747ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2018 ===================\n",
    "\n",
    "# Individuales (obligatoria)\n",
    "TIT_INDIV_2018 = [\n",
    "    \"2018 - Antofagasta - Siniestros Individuales\",\n",
    "    \"2018 - Araucania - Siniestros Individuales\",\n",
    "    \"2018 - Arica y Parinacota - Siniestros Individuales\",\n",
    "    \"2018 - Atacama - Siniestros Individuales\",\n",
    "    \"2018 - Aysén del General Carlos Ibañez del Campo - Siniestros Individuales\",\n",
    "    \"2018 - Biobío - Siniestros Individuales\",\n",
    "    \"2018 - Coquimbo - Siniestros Individuales\",\n",
    "    \"2018 - Libertador Bernardo O'Higgins - Siniestros Individuales\",\n",
    "    \"2018 - Los Lagos - Siniestros Individuales\",\n",
    "    \"2018 - Los Rios - Siniestros Individuales\",\n",
    "    \"2018 - Magallanes y la Antartica Chilena - Siniestros Individuales\",\n",
    "    \"2018 - Maule - Siniestros Individuales\",\n",
    "    \"2018 - Metropolitana - Siniestros Individuales\",\n",
    "    \"2018 - Tarapaca - Siniestros Individuales\",\n",
    "    \"2018 - Valparaíso - Siniestros Individuales\",\n",
    "    \"2018 - Ñuble - Siniestros Individuales\",\n",
    "]\n",
    "\n",
    "TIT_RUTA_2018 = [\n",
    "    \"Siniestros de tránsito en ruta _ 2018\"\n",
    "]\n",
    "\n",
    "catalogo_2018 = {\n",
    "    \"df_Individuales_2018\": TIT_INDIV_2018,  \n",
    "    \"df_ruta_2018\":         TIT_RUTA_2018,  \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32ee44-4778-483f-a67f-719f38eae648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2019 ===================\n",
    "\n",
    "# Individuales (incluye Gran Santiago; O'Higgins viene con título alterno)\n",
    "TIT_INDIV_2019 = [\n",
    "    \"Siniestros de tránsito, región Arica y Parinacota, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Antofagasta, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Atacama, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Aysén, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Coquimbo, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de La Araucanía, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Los Lagos, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Los Ríos, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Magallanes, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Tarapacá, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Valparaíso, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región de Ñuble, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región del Biobío, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, región del Maule, Chile, 2019.\",\n",
    "    \"Siniestros de tránsito, Gran Santiago, RM, Chile, 2019.\",\n",
    "    \"2019 - Libertador Bernardo O'Higgins - Siniestros Individuales\",  # título alterno\n",
    "\n",
    "]\n",
    "\n",
    "# Ruta \n",
    "TIT_RUTA_2019 = [\n",
    "    \"Siniestros de tránsito en ruta 2019\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "catalogo_2019 = {\n",
    "    \"df_Individuales_2019\": TIT_INDIV_2019,  \n",
    "    \"df_ruta_2019\":         TIT_RUTA_2019,   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d4d8-3bb0-4393-bcb7-12eeea8786dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2021 ===================\n",
    "\n",
    "# Individuales (16 regiones)\n",
    "TIT_INDIV_2021 = [\n",
    "    \"2021 - Antofagasta - Siniestros Individuales\",\n",
    "    \"2021 - Araucania - Siniestros Individuales\",\n",
    "    \"2021 - Arica y Parinacota - Siniestros Individuales\",\n",
    "    \"2021 - Atacama - Siniestros Individuales\",\n",
    "    \"2021 - Aysén del General Carlos Ibañez del Campo - Siniestros Individuales\",\n",
    "    \"2021 - Biobío - Siniestros Individuales\",\n",
    "    \"2021 - Coquimbo - Siniestros Individuales\",\n",
    "    \"2021 - Libertador Bernardo O'higgins - Siniestros Individuales\",\n",
    "    \"2021 - Los Lagos - Siniestros Individuales\",\n",
    "    \"2021 - Los Ríos - Siniestros Individuales\",\n",
    "    \"2021 - Magallanes y La Antartica Chilena - Siniestros Individuales\",\n",
    "    \"2021 - Maule - Siniestros Individuales\",\n",
    "    \"2021 - Metropolitana de Santiago - Siniestros Individuales\",\n",
    "    \"2021 - Tarapacá - Siniestros Individuales\",\n",
    "    \"2021 - Valparaíso - Siniestros Individuales\",\n",
    "    \"2021 - Ñuble - Siniestros Individuales\",\n",
    "]\n",
    "\n",
    "# Ruta \n",
    "TIT_RUTA_2021 = [\n",
    "    \"Siniestros de tránsito en ruta 2021\",\n",
    "]\n",
    "\n",
    "catalogo_2021 = {\n",
    "    \"df_Individuales_2021\": TIT_INDIV_2021,  \n",
    "    \"df_ruta_2021\":         TIT_RUTA_2021  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b449e76-7cfc-4819-beda-4b297e733bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2022 ===================\n",
    "\n",
    "# Individuales (16 regiones) \n",
    "TIT_INDIV_2022 = [\n",
    "    \"2022 -  Atacama - Siniestros Individuales\",\n",
    "    \"2022 -  Biobio - Siniestros Individuales\",\n",
    "    \"2022 - Antofagasta - Siniestros Individuales\",\n",
    "    \"2022 - Araucania - Siniestros Individuales\",\n",
    "    \"2022 - Arica y Parinacota - Siniestros Individuales\",\n",
    "    \"2022 - Aysén del General Carlos Ibañez del Campo - Siniestros Individuales\",\n",
    "    \"2022 - Coquimbo - Siniestros Individuales\",\n",
    "    \"2022 - Libertador General Bernardo OHiggins - Siniestros Individuales\",\n",
    "    \"2022 - Los Lagos - Siniestros Individuales\",\n",
    "    \"2022 - Los Rios - Siniestros Individuales\",\n",
    "    \"2022 - Magallanes y la Antartica Chilena - Siniestros Individuales\",\n",
    "    \"2022 - Maule - Siniestros Individuales\",\n",
    "    \"2022 - Metropolitana - Siniestros Individuales\",\n",
    "    \"2022 - Tarapaca - Siniestros Individuales\",\n",
    "    \"2022 - Valparaíso - Siniestros Individuales\",\n",
    "    \"2022 - Ñuble - Siniestros Individuales\",\n",
    "]\n",
    "\n",
    "TIT_RUTA_2022 = [\n",
    "    \"Siniestros de tránsito en ruta 2022\"\n",
    "]\n",
    "\n",
    "catalogo_2022 = {\n",
    "    \"df_Individuales_2022\": TIT_INDIV_2022, \n",
    "    \"df_ruta_2022\":         TIT_RUTA_2022,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7b344-3543-42b2-8fd7-54b3137e255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2023 ===================\n",
    "\n",
    "# Individuales (según tus títulos tal cual)\n",
    "TIT_INDIV_2023 = [\n",
    "    \"SIniestros individuales REGION DEL BIO BIO 2023\",\n",
    "    \"Siniestros individuales REGION ANTOFAGASTA 2023\",\n",
    "    \"Siniestros individuales REGION ATACAMA 2023\",\n",
    "    \"Siniestros individuales REGION AYSEN DEL GRL_ CARLOS IBAÑEZ DEL CAMPO 2023\",\n",
    "    \"Siniestros individuales REGION COQUIMBO 2023\",\n",
    "    \"Siniestros individuales REGION DE LOS RIOS 2023\",\n",
    "    \"Siniestros individuales REGION LA ARAUCANIA 2023\",\n",
    "    \"Siniestros individuales REGION LIB_B_OHIGGINS 2023\",\n",
    "    \"Siniestros individuales REGION LOS LAGOS 2023\",\n",
    "    \"Siniestros individuales REGION MAGALLANES Y ANTARTICA CHILENA 2023\",\n",
    "    \"Siniestros individuales REGION MAULE 2023\",\n",
    "    \"Siniestros individuales REGION METROPOLITANA 2023\",\n",
    "    \"Siniestros individuales REGION TARAPACA 2023\",\n",
    "    \"Siniestros individuales REGION VALPARAISO 2023\",\n",
    "    \"Siniestros individuales REGION ÑUBLE 2023\",\n",
    "]\n",
    "\n",
    "\n",
    "TIT_RUTA_2023 = [\n",
    "    \"Siniestros de tránsito en ruta 2023\"\n",
    "]\n",
    "\n",
    "\n",
    "catalogo_2023 = {\n",
    "    \"df_Individuales_2023\": TIT_INDIV_2023,  \n",
    "    \"df_ruta_2023\":         TIT_RUTA_2023   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc171a30-f781-47fe-890b-23c05a109731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIG 2024 ===================\n",
    "\n",
    "# Individuales (urbanos por región)\n",
    "TIT_INDIV_2024 = [\n",
    "    \"Siniestros_urbanos_Antofagasta_2024\",\n",
    "    \"Siniestros_urbanos_Arica_y_Parinacota_2024\",\n",
    "    \"Siniestros_urbanos_Aysen_2024\",\n",
    "    \"Siniestros_urbanos_biobio_2024\",\n",
    "    \"Siniestros_urbanos_Coquimbo_2024\",\n",
    "    \"Siniestros_urbanos_Los_Lagos_2024\",\n",
    "    \"Siniestros_urbanos_Los_Rios_2024\",\n",
    "    \"Siniestros_urbanos_Magallanes_2024\",\n",
    "    \"Siniestros_urbanos_Maule_2024\",\n",
    "    \"Siniestros_urbanos_Metropolitana_2024\",\n",
    "    \"Siniestros_urbanos_Ohiggins_2024\",\n",
    "    \"Siniestros_urbanos_Tarapaca_2024\",\n",
    "    \"Siniestros_urbanos_Valparaiso_2024\",\n",
    "    \"Siniestros_urbanos_Ñuble_2024\",\n",
    "]\n",
    "\n",
    "# Ruta \n",
    "TIT_RUTA_2024 = [\n",
    "    \"Siniestros de tránsito en rutas 2024\"\n",
    "]\n",
    "\n",
    "\n",
    "catalogo_2024 = {\n",
    "    \"df_Individuales_2024\": TIT_INDIV_2024,  \n",
    "    \"df_ruta_2024\":         TIT_RUTA_2024,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db85c8-e8c3-484e-89c9-c954c9162b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2018, catalogo_2018)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdee809-db5d-4639-ba07-98faf682a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2019, catalogo_2019)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6c144-4b1d-4e45-993a-d9ec7d1cb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2021, catalogo_2021)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1876e7-87ae-4395-b789-e14ddf2acbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2022, catalogo_2022)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b181ca8-3f2b-487b-a163-198707086b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2023, catalogo_2023)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ab380-f37f-4711-bdba-6b18a5b71744",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cols = descargar_y_listar_columnas(2024, catalogo_2024)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db8847-3522-4d6d-a45f-4967dc177de6",
   "metadata": {},
   "source": [
    "Normalizar el nombre de las columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737505c-1e02-4aaf-85c9-16c33af705f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Individuales_2018 = normalize_column_names(df_Individuales_2018) \n",
    "df_Individuales_2019 = normalize_column_names(df_Individuales_2019) \n",
    "df_Individuales_2021 = normalize_column_names(df_Individuales_2021) \n",
    "df_Individuales_2022 = normalize_column_names(df_Individuales_2022) \n",
    "df_Individuales_2023 = normalize_column_names(df_Individuales_2023) \n",
    "df_Individuales_2024 = normalize_column_names(df_Individuales_2024) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32645c3b-fc33-4433-bcd9-d949e755ac5d",
   "metadata": {},
   "source": [
    "Unificamos los distintos nombres de la misma variable según el diccionario de alias detectado en la exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc613c-a2e2-480e-a228-1da614e9e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = conaset_to_7vars(df_Individuales_2018, df_name=\"df_Individuales_2018\")\n",
    "df_2019 = conaset_to_7vars(df_Individuales_2019, df_name=\"df_Individuales_2019\")\n",
    "df_2021 = conaset_to_7vars(df_Individuales_2021, df_name=\"df_Individuales_2021\")\n",
    "df_2022 = conaset_to_7vars(df_Individuales_2022, df_name=\"df_Individuales_2022\")\n",
    "df_2023 = conaset_to_7vars(df_Individuales_2023, df_name=\"df_Individuales_2023\")\n",
    "df_2024 = conaset_to_7vars(df_Individuales_2024, df_name=\"df_Individuales_2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32831c27-093d-44f5-956c-6a2d8fdf14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing(name, df):\n",
    "    n = len(df)\n",
    "    miss = df.isna().sum()\n",
    "    miss = miss[miss > 0].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{name}  |  filas={n}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if miss.empty:\n",
    "        print(\"✓ Sin datos faltantes (todas las columnas completas).\")\n",
    "    else:\n",
    "        print(\"Columnas con faltantes:\")\n",
    "        for col, cnt in miss.items():\n",
    "            pct = (cnt / n * 100) if n else float(\"nan\")\n",
    "            print(f\"  - {col}: {cnt} ({pct:.1f}%)\")\n",
    "\n",
    "# Ejecutar para tus dfs\n",
    "report_missing(\"df_2018\", df_2018)\n",
    "report_missing(\"df_2019\", df_2019)\n",
    "report_missing(\"df_2021\", df_2021)\n",
    "report_missing(\"df_2022\", df_2022)\n",
    "report_missing(\"df_2023\", df_2023)\n",
    "report_missing(\"df_2024\", df_2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32199fe-29ea-4e7f-b7dc-6af13a108575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Ejecucion reportes con tus DF\n",
    "# ------------------------------\n",
    "nombres = [\"df_2018\", \"df_2019\",\"df_2021\",\"df_2022\",\"df_2023\",\"df_2024\"\n",
    "]\n",
    "\n",
    "# a) Chequeo intra-DF (filas con idaccident duplicado en el mismo DF)\n",
    "for _name in nombres:\n",
    "    try:\n",
    "        _df = globals()[_name]\n",
    "        report_idaccident_dupes(_name, _df)\n",
    "    except KeyError:\n",
    "        pass  # ese DF no existe en el entorno\n",
    "\n",
    "# b) Chequeo inter-DF (ids que se repiten entre distintos DFs)\n",
    "df_dict_presentes = {name: globals()[name] for name in nombres if name in globals()}\n",
    "report_cross_df_overlaps(df_dict_presentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb28c0e-84cf-4567-a53e-8e7e8b7cd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar para tus dfs\n",
    "report_missing(\"df_2018\", df_2018)\n",
    "report_missing(\"df_2019\", df_2019)\n",
    "report_missing(\"df_2021\", df_2021)\n",
    "report_missing(\"df_2022\", df_2022)\n",
    "report_missing(\"df_2023\", df_2023)\n",
    "report_missing(\"df_2024\", df_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71a611-47cb-4f47-a4c3-f5189352ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMBRES = [\"df_2018\", \"df_2019\", \"df_2021\", \"df_2022\", \"df_2023\", \"df_2024\"]\n",
    "\n",
    "# 1) Tomar los DFs que existan en el entorno\n",
    "_presentes = [globals()[n] for n in NOMBRES if n in globals()]\n",
    "if not _presentes:\n",
    "    raise RuntimeError(\"No se encontró ningún DataFrame de la lista NOMBRES.\")\n",
    "\n",
    "# 2) Asegurar mismo orden de columnas (usamos el del primero como referencia)\n",
    "cols_ref = list(_presentes[0].columns)\n",
    "\n",
    "# (Opcional) chequeo rápido de que todos tienen EXACTAMENTE las mismas columnas\n",
    "for i, df in enumerate(_presentes, start=1):\n",
    "    if list(df.columns) != cols_ref:\n",
    "        raise ValueError(f\"El DataFrame #{i} no tiene las mismas columnas ni en el mismo orden.\")\n",
    "\n",
    "# 3) Reindexar por seguridad (por si algún DF viene con orden distinto) y concatenar\n",
    "_presentes = [df.reindex(columns=cols_ref) for df in _presentes]\n",
    "df_accidentes = pd.concat(_presentes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c636c90-0a0d-4387-ad13-951ebff54f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidentes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e5b75-3024-452c-b5e5-4779817c503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_accidentes['idaccident'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fec08b-eba4-4cd9-b8d1-5a083e40013e",
   "metadata": {},
   "source": [
    "Control de nulos: verificar nulos en claves e imputar/asignar por ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26609a69-6d3b-4eb8-ac4b-b020bf2bb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, chk = reemplazar_etiqueta_por_id(\n",
    "    df=df_accidentes,\n",
    "    catalogo=df_codigos,              # tu tabla maestra 1↔1\n",
    "    col_codigo=\"region_id\",\n",
    "    col_etiqueta=\"region\",\n",
    "    cat_codigo=\"Código Región\",\n",
    "    cat_etiqueta=\"Nombre Región\",\n",
    "    solo_si_vacia=False,              # pon True si quieres rellenar solo los vacíos\n",
    "    imprimir=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408d8d4-19de-454a-adec-1b987b722a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# únicos en df_accidentes['region'] que NO están en df_codigos['Nombre Región']\n",
    "u_acc = set(df_accidentes['region'].dropna().astype('string').str.strip().unique())\n",
    "u_cat = set(df_codigos['Nombre Región'].dropna().astype('string').str.strip().unique())\n",
    "\n",
    "faltan = sorted(u_acc - u_cat)\n",
    "print(f\"No presentes en catálogo: {len(faltan)}\")\n",
    "if len(faltan) <= 50:\n",
    "    print(faltan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344c709-1be3-478b-96bf-ca9e644762d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiones_estandar = {\n",
    "    'REGION ANTOFAGASTA': 'Antofagasta',\n",
    "    'REGION ARAUCANIA': 'La Araucanía',\n",
    "    'REGION ARICA Y PARINACOTA': 'Arica y Parinacota',\n",
    "    'REGION AYSEN DEL GRL. CARLOS IBANEZ DEL CAMPO': 'Aysén del General Carlos Ibáñez del Campo',\n",
    "    'REGION BIO BIO': 'Biobío',\n",
    "    'REGION COQUIMBO': 'Coquimbo',\n",
    "    'REGION DE LOS RIOS': 'Los Ríos',\n",
    "    \"REGION LIB.B.O'HIGGINS\": \"Libertador General Bernardo O'Higgins\",\n",
    "    'REGION LOS LAGOS': 'Los Lagos',\n",
    "    'REGION MAGALLANES Y ANTARTICA CHILENA': 'Magallanes y de la Antártica Chilena',\n",
    "    'REGION MAULE': 'Maule',\n",
    "    'REGION METROPOLITANA': 'Metropolitana de Santiago',\n",
    "    'REGION NUBLE': 'Ñuble',\n",
    "    'REGION TARAPACA': 'Tarapacá',\n",
    "    'REGION VALPARAISO': 'Valparaíso',\n",
    "}\n",
    "\n",
    "df_accidentes['region'] = (\n",
    "    df_accidentes['region'].astype('string').str.strip().replace(regiones_estandar)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cd7d2-a272-4d8b-ba28-8a52b3431630",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, chk = reemplazar_etiqueta_por_id(\n",
    "    df=df_accidentes,\n",
    "    catalogo=df_codigos,              # tu tabla maestra 1↔1\n",
    "    col_codigo=\"comuna_id\",\n",
    "    col_etiqueta=\"comuna\",\n",
    "    cat_codigo=\"Código Comuna 2018\",\n",
    "    cat_etiqueta=\"Nombre Comuna\",\n",
    "    solo_si_vacia=False,              # pon True si quieres rellenar solo los vacíos\n",
    "    imprimir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54351ebf-2b21-4c03-849e-265b3c7ccbd3",
   "metadata": {},
   "source": [
    "ELiminamos columnas que ya no necesitamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684a1cd-1579-4336-a2bf-8a5293c28245",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar = [\"region_id\", \"comuna_id\"]\n",
    "df_accidentes = df_accidentes.drop(columns=eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e908ca7-7520-4df9-b4b0-e22a7876c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing(\"df_accidentes\", df_accidentes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea007c74-705c-485e-9c7f-50334d10b84a",
   "metadata": {},
   "source": [
    "Comenzamos con los df_ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baceb738-9f9e-4f44-9beb-be665b634797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_ruta = conaset_ruta_to_7vars(df_ruta_2018, df_name=\"df_ruta_2018\")\n",
    "df_2019_ruta = conaset_ruta_to_7vars(df_ruta_2019, df_name=\"df_ruta_2019\")\n",
    "df_2021_ruta = conaset_ruta_to_7vars(df_ruta_2021, df_name=\"df_ruta_2021\")\n",
    "df_2022_ruta = conaset_ruta_to_7vars(df_ruta_2022, df_name=\"df_ruta_2022\")\n",
    "df_2023_ruta = conaset_ruta_to_7vars(df_ruta_2023, df_name=\"df_ruta_2023\")\n",
    "df_2024_ruta = conaset_ruta_to_7vars(df_ruta_2024, df_name=\"df_ruta_2024\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759190c-bd2f-43fc-9658-676d9315f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Ejecucion reportes con tus DF\n",
    "# ------------------------------\n",
    "nombres = [\"df_2018_ruta\",\n",
    "    \"df_2019_ruta\",\"df_2021_ruta\",\"df_2022_ruta\",\"df_2023_ruta\",\"df_2024_ruta\"\n",
    "]\n",
    "\n",
    "# a) Chequeo intra-DF (filas con idaccident duplicado en el mismo DF)\n",
    "for _name in nombres:\n",
    "    try:\n",
    "        _df = globals()[_name]\n",
    "        report_idaccident_dupes(_name, _df)\n",
    "    except KeyError:\n",
    "        pass  # ese DF no existe en el entorno\n",
    "\n",
    "# b) Chequeo inter-DF (ids que se repiten entre distintos DFs)\n",
    "df_dict_presentes = {name: globals()[name] for name in nombres if name in globals()}\n",
    "report_cross_df_overlaps(df_dict_presentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b78ddc-c69a-4734-88e5-dfd7c0bba434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing(name, df):\n",
    "    n = len(df)\n",
    "    miss = df.isna().sum()\n",
    "    miss = miss[miss > 0].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{name}  |  filas={n}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if miss.empty:\n",
    "        print(\"✓ Sin datos faltantes (todas las columnas completas).\")\n",
    "    else:\n",
    "        print(\"Columnas con faltantes:\")\n",
    "        for col, cnt in miss.items():\n",
    "            pct = (cnt / n * 100) if n else float(\"nan\")\n",
    "            print(f\"  - {col}: {cnt} ({pct:.1f}%)\")\n",
    "\n",
    "# Ejecucion\n",
    "report_missing(\"df_2019_ruta\", df_2019_ruta)\n",
    "report_missing(\"df_2021_ruta\", df_2021_ruta)\n",
    "report_missing(\"df_2022_ruta\", df_2022_ruta)\n",
    "report_missing(\"df_2023_ruta\", df_2023_ruta)\n",
    "report_missing(\"df_2024_ruta\", df_2024_ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67a56e-242b-457b-8e41-33c7263c15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMBRES = [\"df_2018_ruta\",\"df_2019_ruta\",\"df_2021_ruta\",\"df_2022_ruta\",\"df_2023_ruta\",\"df_2024_ruta\"]\n",
    "\n",
    "# 1) Tomar los df que existan en el entorno\n",
    "_presentes = [globals()[n] for n in NOMBRES if n in globals()]\n",
    "if not _presentes:\n",
    "    raise RuntimeError(\"No se encontró ningún DataFrame de la lista NOMBRES.\")\n",
    "\n",
    "# 2) Asegurar mismo orden de columnas (usamos el del primero como referencia)\n",
    "cols_ref = list(_presentes[0].columns)\n",
    "\n",
    "# (Opcional) chequeo rápido de que todos tienen EXACTAMENTE las mismas columnas\n",
    "for i, df2 in enumerate(_presentes, start=1):\n",
    "    if list(df2.columns) != cols_ref:\n",
    "        raise ValueError(f\"El DataFrame #{i} no tiene las mismas columnas ni en el mismo orden.\")\n",
    "\n",
    "# 3) Reindexar por seguridad (por si algún DF viene con orden distinto) y concatenar\n",
    "_presentes = [df2.reindex(columns=cols_ref) for df in _presentes]\n",
    "df_rutas = pd.concat(_presentes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b668deb-9be7-4592-a3e3-f3aa1766b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rutas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3301fc5-9206-4df3-b96d-33ef1aa7b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, chk = reemplazar_etiqueta_por_id(\n",
    "    df=df_rutas,\n",
    "    catalogo=df_codigos,              # tu tabla maestra 1↔1\n",
    "    col_codigo=\"region_id\",\n",
    "    col_etiqueta=\"region\",\n",
    "    cat_codigo=\"Código Región\",\n",
    "    cat_etiqueta=\"Nombre Región\",\n",
    "    solo_si_vacia=False,              # pon True si quieres rellenar solo los vacíos\n",
    "    imprimir=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729d374-15d4-42b2-8618-859e79151f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rutas['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab40fe4-42d2-4c55-a1b2-1137db54d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar = [\"region_id\", \"comuna_id\"]\n",
    "df_rutas = df_rutas.drop(columns=eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b81fb0-f521-4552-945f-bafcafc97e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rutas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd0e2-9f49-45b5-9fc6-c4c7b2305b5b",
   "metadata": {},
   "source": [
    "Unificación final y exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd37e2-134d-4cc5-aa81-baa4dae573fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NOMBRES = ['df_rutas','df_accidentes']\n",
    "OUTPUT_PATH = \"accidentes_chile.parquet\"\n",
    "\n",
    "# --- 1) Reunir los DFs presentes en el entorno ---\n",
    "presentes = [globals()[n] for n in NOMBRES if n in globals()]\n",
    "if not presentes:\n",
    "    raise RuntimeError(\"No se encontró ningún DataFrame de la lista definida.\")\n",
    "\n",
    "# --- 2) Alinear por columnas comunes (intersección) ---\n",
    "sets_cols = [set(df.columns) for df in presentes]\n",
    "cols_comunes = sorted(set.intersection(*sets_cols))\n",
    "if not cols_comunes:\n",
    "    raise RuntimeError(\"Los DataFrames no comparten columnas en común; no se puede concatenar.\")\n",
    "\n",
    "dfs_alineados = [df[cols_comunes] for df in presentes]  \n",
    "\n",
    "# --- 3) Concatenar (Pandas respetará los dtypes comunes; no forzamos conversiones) ---\n",
    "df_all = pd.concat(dfs_alineados, ignore_index=True)\n",
    "\n",
    "# (Opcional) Verificar que idaccident se mantuvo como float\n",
    "if \"idaccident\" in df_all.columns:\n",
    "    print(\"dtype idaccident:\", df_all[\"idaccident\"].dtype)  \n",
    "\n",
    "# --- 4) Exportar a Parquet (recomendado pyarrow instalado) ---\n",
    "df_all.to_parquet(OUTPUT_PATH, index=False)  \n",
    "print(f\"✅ Exportado: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9777e2e-e37f-437b-b851-cac59aae3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233a8fc-13cb-4b3e-be32-d76a828c971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['año'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e2ff8-819c-402d-bb09-6d7235d41333",
   "metadata": {},
   "source": [
    "Seccion de Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba3a47-5dd8-4cc2-a61e-df6359bb2a53",
   "metadata": {},
   "source": [
    "Pipeline para descargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf31a2-bff8-410c-a32a-5e4602bd03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Pipeline multi-categoría: totales por región (solo imprime totales) =====\n",
    "ARCGIS_SEARCH = \"https://www.arcgis.com/sharing/rest/search\"\n",
    "TIMEOUT = 60\n",
    "CHUNK = 2000\n",
    "DROP_COLS = {\"KM_MARKER\",\"LAT\",\"LONG\",\"CONDICION_VIA\"}\n",
    "\n",
    "def _make_hdr(year:int):\n",
    "    return {\"User-Agent\": f\"conaset-{year}/mini\"}\n",
    "\n",
    "def _search_urls(q, hdr, num=80):\n",
    "    p = {\"f\":\"json\",\"q\":q,\"num\":num,\"sortField\":\"title\",\"sortOrder\":\"asc\"}\n",
    "    r = requests.get(ARCGIS_SEARCH, params=p, headers=hdr, timeout=TIMEOUT); r.raise_for_status()\n",
    "    return [it.get(\"url\",\"\") for it in (r.json().get(\"results\") or []) if it.get(\"url\")]\n",
    "\n",
    "def _layer_urls(svc):\n",
    "    m = re.search(r\"/(FeatureServer|MapServer)/(\\d+)\\b\", svc)\n",
    "    if m: return [svc]\n",
    "    if   \"FeatureServer\" in svc: base = svc.split(\"FeatureServer\")[0]+\"FeatureServer\"\n",
    "    elif \"MapServer\"   in svc:   base = svc.split(\"MapServer\")[0]+\"MapServer\"\n",
    "    else:                        base = svc.rstrip(\"/\")+\"/FeatureServer\"\n",
    "    j = requests.get(base, params={\"f\":\"json\"}, timeout=TIMEOUT).json()\n",
    "    layers = j.get(\"layers\") or []\n",
    "    return [f\"{base}/{ly['id']}\" for ly in layers if 'id' in ly] or [svc]\n",
    "\n",
    "def _drop_cols(df: pd.DataFrame):\n",
    "    if df.empty: return df\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in DROP_COLS:\n",
    "        cc = low.get(c.lower())\n",
    "        if cc in df.columns: df = df.drop(columns=[cc])\n",
    "    return df\n",
    "\n",
    "def _fetch_layer(lurl, hdr):\n",
    "    off, out = 0, []\n",
    "    while True:\n",
    "        p = {\"f\":\"json\",\"where\":\"1=1\",\"outFields\":\"*\",\"returnGeometry\":\"false\",\n",
    "             \"resultOffset\":off,\"resultRecordCount\":CHUNK}\n",
    "        r = requests.get(f\"{lurl}/query\", params=p, headers=hdr, timeout=TIMEOUT); r.raise_for_status()\n",
    "        js = r.json(); feats = js.get(\"features\") or []\n",
    "        if not feats: break\n",
    "        out.extend([f[\"attributes\"] for f in feats])\n",
    "        if js.get(\"exceededTransferLimit\") or len(feats)==CHUNK:\n",
    "            off += CHUNK\n",
    "            time.sleep(0.03)\n",
    "        else:\n",
    "            break\n",
    "    return _drop_cols(pd.DataFrame.from_records(out))\n",
    "\n",
    "# --- Obtención del nombre de región desde el título (robusto a formatos) ---\n",
    "def _region_from_title(title:str, year:int):\n",
    "    m = re.match(rf\"\\s*{year}\\s*-\\s*([^-\\n\\r]+?)\\s*-\\s*\", title, flags=re.IGNORECASE)\n",
    "    if m: return m.group(1).strip()\n",
    "    m = re.search(r\"región(?: de| del)?\\s+([^,]+)\", title, flags=re.IGNORECASE)\n",
    "    if m: return m.group(1).strip()\n",
    "    # “Gran Santiago, RM ...” u otros: tomar el primer bloque que luzca a región\n",
    "    parts = [p.strip() for p in re.split(r\"[,-]\", title) if p.strip()]\n",
    "    if parts: return parts[0]\n",
    "    return title.strip()\n",
    "\n",
    "def _download_by_title(title:str, year:int, hdr, owner=\"conaset_mtt\"):\n",
    "    \"\"\"Devuelve un DataFrame para 1 título (puede unir varias capas de ese título).\"\"\"\n",
    "    q1 = f'owner:{owner} (type:\"Feature Layer\" OR type:\"Feature Service\") title:\"{title}\"'\n",
    "    # fallback más laxo: quita el año del texto base\n",
    "    key = (title.replace(\"Siniestros de tránsito\",\"Siniestros\")\n",
    "                 .replace(\"Incidentes de tránsito\",\"Incidentes\")\n",
    "                 .replace(\"  \",\" \"))\n",
    "    base_key = key.replace(str(year), \"\").strip().strip(\",.- \")\n",
    "    q2 = f'owner:{owner} (type:\"Feature Layer\" OR type:\"Feature Service\") {year} \"{base_key}\"'\n",
    "    # alternativa “Incidentes”\n",
    "    alt = title.replace(\"Siniestros\",\"Incidentes\")\n",
    "    q3 = f'owner:{owner} (type:\"Feature Layer\" OR type:\"Feature Service\") title:\"{alt}\"'\n",
    "\n",
    "    urls = _search_urls(q1, hdr) or _search_urls(q2, hdr) or _search_urls(q3, hdr)\n",
    "    if not urls:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    for u in urls:\n",
    "        for lu in _layer_urls(u):\n",
    "            try:\n",
    "                dfi = _fetch_layer(lu, hdr)\n",
    "                if not dfi.empty:\n",
    "                    out = pd.concat([out, dfi], ignore_index=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "def descargar_y_listar_columnas(year:int, catalogo:dict, owner:str=\"conaset_mtt\",\n",
    "                                asignar_globales:bool=True):\n",
    "    \"\"\"\n",
    "    Descarga cada categoría del catálogo y SOLO imprime el nombre de todas las columnas\n",
    "    del DataFrame concatenado por categoría (sin totales, sin detalles).\n",
    "    Crea variables globales df_<categoria> si asignar_globales=True.\n",
    "    Retorna: dict {categoria: DataFrame}\n",
    "    \"\"\"\n",
    "    hdr = _make_hdr(year)\n",
    "    dfs_por_categoria = {}\n",
    "\n",
    "    for categoria, titulos in catalogo.items():\n",
    "        if not titulos:\n",
    "            continue\n",
    "\n",
    "        dfs_cat = []\n",
    "        for t in titulos:\n",
    "            dfi = _download_by_title(t, year, hdr, owner=owner)\n",
    "            if not dfi.empty:\n",
    "                dfs_cat.append(dfi)\n",
    "\n",
    "        # concatenado por categoría (puede ser vacío)\n",
    "        df_cat_total = pd.concat(dfs_cat, ignore_index=True) if dfs_cat else pd.DataFrame()\n",
    "        dfs_por_categoria[categoria] = df_cat_total\n",
    "\n",
    "        # exponer como variable global si se pide\n",
    "        if asignar_globales:\n",
    "            globals()[categoria] = df_cat_total\n",
    "\n",
    "        # imprimir SOLO columnas\n",
    "        cols = list(df_cat_total.columns)\n",
    "        print(f\"{categoria} — columnas ({len(cols)}):\")\n",
    "        if cols:\n",
    "            print(\"  \" + \", \".join(cols))\n",
    "        else:\n",
    "            print(\"  <sin datos>\")\n",
    "\n",
    "    return dfs_por_categoria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c51e6-4c2a-486a-a2b0-26e0063f3f3d",
   "metadata": {},
   "source": [
    "Unificacion de los distintas columnas de cada dataframe para siniestros individuales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb2826-92c2-4666-aa7a-0447023dc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIAS = {\n",
    "    # Identificador\n",
    "    \"idaccident\": [\n",
    "        \"idaccident\",\"id_accident\",\"idsiniestro\",\"id_siniestro\",\"idaccidente\", \"idaccident_2\"\n",
    "    ],\n",
    "\n",
    "    # Región / Comuna (códigos)\n",
    "    \"region_id\": [\n",
    "        \"cod_region\",\"codreg\",\"cod_reg\",\"cod_regi\",\"region_id\",\"codigo_region\", \"codregion\", \"codregion_2\", \"codregi\"\n",
    "    ],\n",
    "    \"comuna_id\": [\n",
    "        \"cod_comuna\",\"codcomuna\",\"codcom\",\"cod_comun\",\"codigo_comuna\",\"cut\", \"cod_comun_2\"\n",
    "    ],\n",
    "\n",
    "    # Fecha/Hora\n",
    "    \"fecha\": [\n",
    "        \"fecha\",\"fechahora\",\"fecha_hora\",\"fechayhora\",\"fecha_siniestro\",\"fch\"\n",
    "    ],\n",
    "\n",
    "    # Etiquetas finales o fuentes para inferirlas\n",
    "    \"tipo_cona\": [\"tipo_cona\"],\n",
    "    \"tipo_detalle\": [\n",
    "        \"tipo_accid\",\"tipo_accidente\",\"tipo_accdt\",\"tipo_accdte\",\"tipo\",\"tpo_accid\"\n",
    "    ],\n",
    "    \"tipo_codigo\": [\"tipoaccdte\",\"cod_tipo\",\"codigo_tipo\"],  # numérico si existe\n",
    "\n",
    "    \"causa_con\": [\"causa_con\"],\n",
    "    \"causa_detalle\": [\"causa\",\"causaacc\",\"causa_siniestro\",\"causa_detalle\"],\n",
    "\n",
    "    \"region\": [\"region\",\"glosa_region\",\"nombre_region\",\"nom_region\",\"region_2\",\"region_1\"],\n",
    "    \"comuna\": [\"comuna\",\"nombre_comuna\",\"nom_comuna\",\"glosa_comuna\",\"comuna_2\",\"comuna_1\",\"comuna\"],\n",
    "    \"calle\": [\n",
    "        \"calle_uno\",\"calleuno\",\"calle\",\"via\",\"via_principal\",\"direccion\",\"nombre_via\",\n",
    "         \"calle1\",\"calle_un_1\"\n",
    "    ],\n",
    "    \"interseccion\": [\n",
    "        \"interseccion\",\"intersec\",\"interseccion_con\",\n",
    "        \"via_intersecta\",\"via_interseccion\",\"tramo\",\"intersecci\"\n",
    "    ],\n",
    "    \"zona\": [\"zona\",\"sector\",\"area\"],\n",
    "    \"ano\": [\"ano\",\"anio\",\"year\",\"ano_siniestro\",\"ano_2\", \"a_o\",\"Año\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb024b-8be5-4c24-a7dd-6cc8a2c97c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_norm_cols(df):\n",
    "    \"\"\"Por si llegara un df sin normalizar: ascii+lower+underscores.\"\"\"\n",
    "    ren = {}\n",
    "    for c in df.columns:\n",
    "        s = unicodedata.normalize(\"NFKD\", str(c)).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "        s = s.lower().strip()\n",
    "        s = re.sub(r\"\\s+\", \"_\", s)\n",
    "        s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "        s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "        ren[c] = s\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "def _first_datetime(df, cand_names):\n",
    "    for c in cand_names:\n",
    "        if c in df.columns:\n",
    "            return pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# === (2) REEMPLAZA DENTRO DE conaset_to_7vars SOLO LAS PARTES SEÑALADAS ===\n",
    "def conaset_to_7vars(df, df_name=None):\n",
    "    df = _safe_norm_cols(df.copy())\n",
    "\n",
    "    # --- Campos base (EXTENDIDO) ---\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"idaccident\"] = coalesce_num(df, ALIAS[\"idaccident\"], dtype=\"Int64\")\n",
    "    out[\"region_id\"]  = coalesce_num(df, ALIAS[\"region_id\"],  dtype=\"Int64\")\n",
    "    out[\"region\"]     = coalesce_str(df, ALIAS[\"region\"])\n",
    "    out[\"comuna_id\"]  = coalesce_num(df, ALIAS[\"comuna_id\"],  dtype=\"Int64\")\n",
    "    out[\"comuna\"]     = coalesce_str(df, ALIAS[\"comuna\"])\n",
    "    if \"comunareal\" in df.columns:\n",
    "        comuna_real = coalesce_str(df, [\"comunareal\"])\n",
    "        mask = out[\"comuna\"].isna() | out[\"comuna\"].astype(\"string\").str.strip().eq(\"\")\n",
    "        if mask.any():\n",
    "            out.loc[mask, \"comuna\"] = comuna_real\n",
    "\n",
    "    out[\"fecha\"]      = coalesce_datetime(df, ALIAS[\"fecha\"])\n",
    "\n",
    "    out[\"ano\"]        = coalesce_num(df, ALIAS[\"ano\"], dtype=\"Int64\")\n",
    "\n",
    "    # --- Vías y zona (SUSTITUYE el bloque de 'direccion') ---\n",
    "    out[\"calle\"]        = coalesce_str(df, ALIAS[\"calle\"])\n",
    "    out[\"interseccion\"] = coalesce_str(df, ALIAS[\"interseccion\"])\n",
    "    out[\"zona\"]         = coalesce_str(df, ALIAS[\"zona\"])\n",
    "\n",
    "    # --- TIPO_CONA (multi-fuente para aprender y aplicar) ---\n",
    "    tipo_cona_raw = coalesce_str(df, ALIAS[\"tipo_cona\"])\n",
    "    \n",
    "    # Detectar destino presente en el DF para aprender (primera que exista)\n",
    "    tgt_tipo_col = next((c for c in ALIAS[\"tipo_cona\"] if c in df.columns), None)\n",
    "    \n",
    "    # Todas las fuentes disponibles\n",
    "    src_det_cols = [c for c in ALIAS[\"tipo_detalle\"] if c in df.columns]\n",
    "    src_cod_cols = [c for c in ALIAS[\"tipo_codigo\"] if c in df.columns]\n",
    "    \n",
    "    # Aprendizaje 1->1 combinando todas las fuentes\n",
    "    map_tipo_from_det = build_unique_map_str_multi(df, src_det_cols, tgt_tipo_col)\n",
    "    map_tipo_from_cod = build_unique_map_num_to_str_multi(df, src_cod_cols, tgt_tipo_col)\n",
    "    \n",
    "    # Aplicación: primer texto/código no nulo entre todas las fuentes\n",
    "    det_1 = coalesce_str(df, src_det_cols) if src_det_cols else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    cod_1 = coalesce_num(df, src_cod_cols, dtype=\"Int64\") if src_cod_cols else pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "    \n",
    "    # Mapear usando claves normalizadas (para detalle)\n",
    "    tipo_from_det = (_norm_txt(det_1).map(map_tipo_from_det)\n",
    "                     if map_tipo_from_det else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "    # Mapear código->tgt (num)\n",
    "    tipo_from_cod = (cod_1.map(map_tipo_from_cod)\n",
    "                     if map_tipo_from_cod else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "    \n",
    "    # Heurística: solo si hay “detalle” de texto (no vacío) y no hubo mapeo\n",
    "    heur = pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    has_det = det_1.notna() & det_1.astype(\"string\").str.strip().ne(\"\")\n",
    "    need_heur = tipo_cona_raw.isna() & tipo_from_det.isna() & tipo_from_cod.isna() & has_det\n",
    "    heur.loc[need_heur] = det_1.where(need_heur).map(heuristica_tipo_grupo)\n",
    "    \n",
    "    # Unión por prioridad (sin sobreescribir lo ya existente)\n",
    "    tipo_cona = tipo_cona_raw.copy()\n",
    "    for cand in [tipo_from_det, tipo_from_cod, heur]:\n",
    "        fill = tipo_cona.isna() & cand.notna()\n",
    "        tipo_cona = tipo_cona.mask(fill, cand)\n",
    "    out[\"tipo_cona\"] = tipo_cona.astype(\"string\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- CAUSA_CON (multi-fuente para aprender y aplicar) ---\n",
    "    causa_con_raw = coalesce_str(df, ALIAS[\"causa_con\"])\n",
    "    \n",
    "    # Destino presente en DF para aprender\n",
    "    tgt_causa_col = next((c for c in ALIAS[\"causa_con\"] if c in df.columns), None)\n",
    "    \n",
    "    # Todas las fuentes de detalle disponibles\n",
    "    src_causa_cols = [c for c in ALIAS[\"causa_detalle\"] if c in df.columns]\n",
    "    \n",
    "    # Aprendizaje 1->1 combinando todas las fuentes\n",
    "    map_causa = build_unique_map_str_multi(df, src_causa_cols, tgt_causa_col)\n",
    "    \n",
    "    # Aplicación: primer texto no nulo entre todas las fuentes\n",
    "    causa_det_1 = coalesce_str(df, src_causa_cols) if src_causa_cols else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    \n",
    "    # Mapear con claves normalizadas\n",
    "    causa_from_det = (_norm_txt(causa_det_1).map(map_causa)\n",
    "                      if map_causa else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "    \n",
    "    # Relleno sin sobreescribir lo ya existente\n",
    "    causa_con = causa_con_raw.copy()\n",
    "    fill_causa = causa_con.isna() & causa_from_det.notna()\n",
    "    causa_con = causa_con.mask(fill_causa, causa_from_det)\n",
    "    out[\"causa_con\"] = causa_con.astype(\"string\")\n",
    "\n",
    "    # --- Limpiezas suaves (AJUSTE: completar 'ano' desde el nombre del DF) ---\n",
    "    if df_name:\n",
    "        name = str(df_name)\n",
    "        # captura 19xx o 20xx sin requerir \\b (funciona con df_2023, df-2023, df2023, etc.)\n",
    "        m = re.search(r'(?<!\\d)(?:19|20)\\d{2}(?!\\d)', name)\n",
    "        if m:\n",
    "            year = int(m.group(0))\n",
    "            mask = out[\"ano\"].isna()\n",
    "            if mask.any():\n",
    "                out.loc[mask, \"ano\"] = pd.Series(year, index=out.index, dtype=\"Int64\")\n",
    "    \n",
    "    # --- Renombrar salida final: 'ano' -> 'año' ---\n",
    "    out = out.rename(columns={\"ano\": \"año\"})\n",
    "\n",
    "    # --- Orden final ---\n",
    "    out = out[[\n",
    "        \"idaccident\",\n",
    "        \"region_id\",\n",
    "        \"region\",\n",
    "        \"comuna_id\",\n",
    "        \"comuna\",\n",
    "        \"calle\",\n",
    "        \"interseccion\",\n",
    "        \"zona\",\n",
    "        \"fecha\",\n",
    "        \"año\",\n",
    "        \"tipo_cona\",\n",
    "        \"causa_con\",\n",
    "    ]]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9a78a-ab95-44bf-aa2a-4ad2b03cbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) Duplicados dentro de cada DF (tu función)\n",
    "# ------------------------------\n",
    "def report_idaccident_dupes(name, df):\n",
    "    if \"idaccident\" not in df.columns:\n",
    "        return\n",
    "    n = len(df)\n",
    "    s = df[\"idaccident\"]\n",
    "    dup_rows = s.notna() & s.duplicated(keep=False)  # marca todas las filas en grupos duplicados\n",
    "    cnt_rows = int(dup_rows.sum())\n",
    "    if cnt_rows > 0:\n",
    "        pct_rows = (cnt_rows / n * 100) if n else 0.0\n",
    "        dup_ids = s[dup_rows].dropna().nunique()\n",
    "        print(f\"{name}: {cnt_rows} filas con idaccident duplicado ({pct_rows:.2f}%) | {dup_ids} id(s) duplicado(s)\")\n",
    "    else:\n",
    "        print(f\"{name}: sin filas duplicadas por idaccident\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Utilidades para análisis entre DFs\n",
    "# ------------------------------\n",
    "def _ids_set(df):\n",
    "    \"\"\"Devuelve el conjunto de IDs válidos (no nulos) como strings para evitar problemas de tipo.\"\"\"\n",
    "    if \"idaccident\" not in df.columns:\n",
    "        return set()\n",
    "    s = df[\"idaccident\"].dropna()\n",
    "    # Convertimos a string estandarizado (por si vienen mezclados int/str)\n",
    "    s = s.astype(str).str.strip()\n",
    "    return set(s)\n",
    "\n",
    "def report_cross_df_overlaps(df_dict):\n",
    "    \"\"\"\n",
    "    df_dict: dict nombre -> DataFrame\n",
    "    Reporta:\n",
    "      - Total de ids únicos por DF\n",
    "      - Si hay ids que se repiten en más de un DF (y cuántos)\n",
    "      - Matriz de intersecciones (conteo y % respecto del DF de la fila)\n",
    "    \"\"\"\n",
    "    # Conjuntos por DF\n",
    "    id_sets = {name: _ids_set(df) for name, df in df_dict.items()}\n",
    "    sizes = {name: len(s) for name, s in id_sets.items()}\n",
    "\n",
    "    # Lista plana con (id, origen)\n",
    "    rows = []\n",
    "    for name, s in id_sets.items():\n",
    "        rows += [(i, name) for i in s]\n",
    "    if not rows:\n",
    "        print(\"No hay IDs en los DataFrames provistos.\")\n",
    "        return\n",
    "\n",
    "    atlas = pd.DataFrame(rows, columns=[\"idaccident\", \"df_src\"])\n",
    "    counts = atlas.groupby(\"idaccident\")[\"df_src\"].nunique().rename(\"n_df\").reset_index()\n",
    "\n",
    "    # IDs que aparecen en >1 DF\n",
    "    repeated = counts[counts[\"n_df\"] > 1]\n",
    "    n_repeated_ids = len(repeated)\n",
    "\n",
    "    print(\"\\n--- Resumen por DataFrame (IDs únicos) ---\")\n",
    "    for name in df_dict.keys():\n",
    "        print(f\"{name:>12}: {sizes[name]} id(s) únicos\")\n",
    "\n",
    "    print(\"\\n--- Repetidos entre DFs ---\")\n",
    "    if n_repeated_ids == 0:\n",
    "        print(\"✅ No hay idaccident repetidos entre los DataFrames (todos únicos a nivel global).\")\n",
    "    else:\n",
    "        # ¿qué % de los IDs globales se repiten en más de un DF?\n",
    "        total_unique_global = counts.shape[0]\n",
    "        pct_rep_global = (n_repeated_ids / total_unique_global * 100) if total_unique_global else 0.0\n",
    "        print(f\"⚠️  {n_repeated_ids} id(s) aparecen en más de un DataFrame \"\n",
    "              f\"({pct_rep_global:.2f}% de los {total_unique_global} ID únicos globales).\")\n",
    "\n",
    "        # Top ejemplos (opcionales): mostrar hasta 10 ids con los DF donde aparecen\n",
    "        example = (\n",
    "            atlas.merge(repeated[[\"idaccident\"]], on=\"idaccident\", how=\"inner\")\n",
    "                 .groupby(\"idaccident\")[\"df_src\"].agg(lambda x: sorted(set(x)))\n",
    "                 .head(10)\n",
    "        )\n",
    "        print(\"\\nEjemplos de IDs repetidos y los DFs donde aparecen (máx 10):\")\n",
    "        for i, srcs in example.items():\n",
    "            print(f\"  - {i}: {', '.join(srcs)}\")\n",
    "\n",
    "    # Matriz de intersecciones (conteo y % relativo a la fila)\n",
    "    names = list(df_dict.keys())\n",
    "    inter_counts = pd.DataFrame(index=names, columns=names, dtype=\"Int64\")\n",
    "    inter_pct = pd.DataFrame(index=names, columns=names, dtype=\"float\")\n",
    "\n",
    "    for i in names:\n",
    "        for j in names:\n",
    "            if sizes[i] == 0:\n",
    "                inter_counts.loc[i, j] = 0\n",
    "                inter_pct.loc[i, j] = 0.0\n",
    "            else:\n",
    "                inter = len(id_sets[i].intersection(id_sets[j]))\n",
    "                inter_counts.loc[i, j] = inter\n",
    "                inter_pct.loc[i, j] = (inter / sizes[i] * 100.0) if sizes[i] else 0.0\n",
    "\n",
    "    print(\"\\n--- Matriz de intersección (conteo) ---\")\n",
    "    print(inter_counts.fillna(0))\n",
    "\n",
    "    print(\"\\n--- Matriz de intersección (% respecto a la fila) ---\")\n",
    "    print(inter_pct.fillna(0).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e97f54-58c4-4c43-a45c-b68754304cb4",
   "metadata": {},
   "source": [
    "Unificacion de los distintas columnas de cada dataframe para siniestros  Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bbbae-ecae-45c7-970b-4b7ddc427444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === (1) ALIAS para datos de RUTAS ===\n",
    "ALIAS_RUTA = {\n",
    "    # Identificador\n",
    "    \"idaccident\": [\n",
    "        \"idaccident\",\"id_accident\",\"idsiniestro\",\"id_siniestro\",\"idaccidente\"\n",
    "    ],\n",
    "\n",
    "    # Región / Comuna (códigos)\n",
    "    \"region_id\": [\n",
    "        \"cod_region\",\"codreg\",\"cod_reg\",\"cod_regi\",\"region_id\",\"codigo_region\", \"codregion\", \"codregion_2\", \"codregi\"\n",
    "    ],\n",
    "    \"comuna_id\": [\n",
    "        \"cod_comuna\",\"codcomuna\",\"codcom\",\"cod_comun\",\"codigo_comuna\",\"cut\", \"cod_comun_2\", \"val_com\" \n",
    "    ],\n",
    "\n",
    "    # Fecha/Hora\n",
    "    \"fecha\": [\n",
    "        \"fecha\",\"fechahora\",\"fecha_hora\",\"fechayhora\",\"fecha_siniestro\",\"fch\"\n",
    "    ],\n",
    "\n",
    "    # Etiquetas finales o fuentes para inferirlas\n",
    "    \"tipo_cona\": [\"tipo_cona\"],\n",
    "    \"tipo_detalle\": [\n",
    "        \"tipo_accid\",\"tipo_accidente\",\"tipo_accdt\",\"tipo_accdte\",\"tipo\",\"tpo_accid\"\n",
    "    ],\n",
    "    \"tipo_codigo\": [\"tipoaccdte\",\"cod_tipo\",\"codigo_tipo\"],  # numérico si existe\n",
    "\n",
    "    \"causa_con\": [\"causa_con\"],\n",
    "    \"causa_detalle\": [\"causa\",\"causaacc\",\"causa_siniestro\",\"causa_detalle\"],\n",
    "\n",
    "    \"region\": [\"region\",\"glosa_region\",\"nombre_region\",\"nom_region\",\"region_2\",\"region_1\"],\n",
    "    \"comuna\": [\"comuna\",\"nombre_comuna\",\"nom_comuna\",\"glosa_comuna\",\"comuna_2\",\"comuna_1\"],\n",
    "    \"calle\": [\n",
    "        \"calle_uno\",\"calleuno\",\"calle\",\"via\",\"via_principal\",\"direccion\",\"nombre_via\",\n",
    "        \"calle1\",\"calle_un_1\"\n",
    "    ],\n",
    "    \"interseccion\": [\n",
    "        \"calle_dos\",\"calledos\",\"interseccion\",\"intersec\",\"interseccion_con\",\n",
    "        \"via_intersecta\",\"via_interseccion\",\"tramo\",\"intersecci\"\n",
    "    ],\n",
    "    \"zona\": [\"zona\",\"sector\",\"area\"],\n",
    "    \"año\": [\"ano\",\"anio\",\"year\",\"ano_siniestro\",\"ano_2\", \"a_o\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cb4cd-b95e-4440-807b-dc64644867f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conaset_ruta_to_7vars(df, df_name=None):\n",
    "    df = _safe_norm_cols(df.copy())\n",
    "\n",
    "    # --- Campos base (EXTENDIDO) ---\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"idaccident\"] = coalesce_num(df, ALIAS_RUTA[\"idaccident\"], dtype=\"Int64\")\n",
    "    out[\"region_id\"]  = coalesce_num(df, ALIAS_RUTA[\"region_id\"],  dtype=\"Int64\")\n",
    "    out[\"region\"]     = coalesce_str(df, ALIAS_RUTA[\"region\"])\n",
    "    out[\"comuna_id\"]  = coalesce_num(df, ALIAS_RUTA[\"comuna_id\"],  dtype=\"Int64\")\n",
    "    out[\"comuna\"]     = coalesce_str(df, ALIAS_RUTA[\"comuna\"])\n",
    "    out[\"fecha\"]      = _first_datetime(df, ALIAS_RUTA[\"fecha\"])\n",
    "    out[\"año\"]        = coalesce_num(df, ALIAS_RUTA[\"año\"], dtype=\"Int64\")\n",
    "\n",
    "    # --- Vías y zona ---\n",
    "    out[\"calle\"]        = coalesce_str(df, ALIAS_RUTA[\"calle\"])\n",
    "    out[\"interseccion\"] = coalesce_str(df, ALIAS_RUTA[\"interseccion\"])\n",
    "    out[\"zona\"]         = coalesce_str(df, ALIAS_RUTA[\"zona\"])\n",
    "\n",
    "    # --- TIPO_CONA (usa helpers multi-fuente del primer bloque) ---\n",
    "    tipo_cona_raw = coalesce_str(df, ALIAS_RUTA[\"tipo_cona\"])\n",
    "\n",
    "    tgt_tipo_col = next((c for c in ALIAS_RUTA[\"tipo_cona\"] if c in df.columns), None)\n",
    "    src_det_cols = [c for c in ALIAS_RUTA[\"tipo_detalle\"] if c in df.columns]\n",
    "    src_cod_cols = [c for c in ALIAS_RUTA[\"tipo_codigo\"] if c in df.columns]\n",
    "\n",
    "    map_tipo_from_det = build_unique_map_str_multi(df, src_det_cols, tgt_tipo_col)\n",
    "    map_tipo_from_cod = build_unique_map_num_to_str_multi(df, src_cod_cols, tgt_tipo_col)\n",
    "\n",
    "    det_1 = coalesce_str(df, src_det_cols) if src_det_cols else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    cod_1 = coalesce_num(df, src_cod_cols, dtype=\"Int64\") if src_cod_cols else pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "\n",
    "    tipo_from_det = (_norm_txt(det_1).map(map_tipo_from_det)\n",
    "                     if map_tipo_from_det else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "    tipo_from_cod = (cod_1.map(map_tipo_from_cod)\n",
    "                     if map_tipo_from_cod else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "\n",
    "    heur = pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    has_det = det_1.notna() & det_1.astype(\"string\").str.strip().ne(\"\")\n",
    "    need_heur = tipo_cona_raw.isna() & tipo_from_det.isna() & tipo_from_cod.isna() & has_det\n",
    "    heur.loc[need_heur] = det_1.where(need_heur).map(heuristica_tipo_grupo)\n",
    "\n",
    "    tipo_cona = tipo_cona_raw.copy()\n",
    "    for cand in [tipo_from_det, tipo_from_cod, heur]:\n",
    "        fill = tipo_cona.isna() & cand.notna()\n",
    "        tipo_cona = tipo_cona.mask(fill, cand)\n",
    "    out[\"tipo_cona\"] = tipo_cona.astype(\"string\")\n",
    "\n",
    "    # --- CAUSA_CON (multi-fuente como en el primer bloque) ---\n",
    "    causa_con_raw = coalesce_str(df, ALIAS_RUTA[\"causa_con\"])\n",
    "\n",
    "    tgt_causa_col = next((c for c in ALIAS_RUTA[\"causa_con\"] if c in df.columns), None)\n",
    "    src_causa_cols = [c for c in ALIAS_RUTA[\"causa_detalle\"] if c in df.columns]\n",
    "\n",
    "    map_causa = build_unique_map_str_multi(df, src_causa_cols, tgt_causa_col)\n",
    "\n",
    "    causa_det_1 = coalesce_str(df, src_causa_cols) if src_causa_cols else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "\n",
    "    causa_from_det = (_norm_txt(causa_det_1).map(map_causa)\n",
    "                      if map_causa else pd.Series(pd.NA, index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "\n",
    "    causa_con = causa_con_raw.copy()\n",
    "    fill_causa = causa_con.isna() & causa_from_det.notna()\n",
    "    causa_con = causa_con.mask(fill_causa, causa_from_det)\n",
    "    out[\"causa_con\"] = causa_con.astype(\"string\")\n",
    "\n",
    "    # --- Limpiezas suaves año <-> fecha ---\n",
    "    mask_a = out[\"año\"].isna() & out[\"fecha\"].notna()\n",
    "    out.loc[mask_a, \"año\"] = out.loc[mask_a, \"fecha\"].dt.year.astype(\"Int64\")\n",
    "\n",
    "    mask_b = out[\"fecha\"].isna() & out[\"año\"].notna()\n",
    "    out.loc[mask_b, \"fecha\"] = pd.to_datetime(\n",
    "        out.loc[mask_b, \"año\"].astype(\"Int64\").astype(str) + \"-01-01\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Completar año desde df_name (igual idea que en conaset_to_7vars)\n",
    "    if df_name:\n",
    "        name = str(df_name)\n",
    "        m = re.search(r'(?<!\\d)(?:19|20)\\d{2}(?!\\d)', name)\n",
    "        if m:\n",
    "            year = int(m.group(0))\n",
    "            mask = out[\"año\"].isna()\n",
    "            if mask.any():\n",
    "                out.loc[mask, \"año\"] = pd.Series(year, index=out.index, dtype=\"Int64\")\n",
    "\n",
    "    # --- Orden final ---\n",
    "    out = out[[\n",
    "        \"idaccident\",\n",
    "        \"region_id\",\n",
    "        \"region\",\n",
    "        \"comuna_id\",\n",
    "        \"comuna\",\n",
    "        \"calle\",\n",
    "        \"interseccion\",\n",
    "        \"zona\",\n",
    "        \"fecha\",\n",
    "        \"año\",\n",
    "        \"tipo_cona\",\n",
    "        \"causa_con\",\n",
    "    ]]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b709a6e-65af-4a6a-8755-6d13a4486d26",
   "metadata": {},
   "source": [
    "Control de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1eba6f-5933-445c-90c7-b25b46f6a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapear_codigos(df, code_col, name_col, strict=False):\n",
    "    # --- Normalización ligera del nombre (IN-PLACE) ---\n",
    "    df[name_col] = df[name_col].astype(\"string\").str.strip().replace({\"\": pd.NA})\n",
    "\n",
    "    # 1) Chequeos rápidos (nombre->multi código, código->multi nombre)\n",
    "    bad_names = df.dropna(subset=[name_col]).groupby(name_col)[code_col].nunique().gt(1)\n",
    "    bad_codes = df.dropna(subset=[code_col]).groupby(code_col)[name_col].nunique().gt(1)\n",
    "\n",
    "    nombres_conflictivos = bad_names[bad_names].index.tolist()\n",
    "    codigos_conflictivos = bad_codes[bad_codes].index.tolist()\n",
    "\n",
    "    if strict and (nombres_conflictivos or codigos_conflictivos):\n",
    "        raise ValueError(\"Conflictos: nombres→multi código o códigos→multi nombre.\")\n",
    "\n",
    "    # 2) Diccionario {codigo: nombre} usando el nombre más frecuente (modo)\n",
    "    mapping = (\n",
    "        df.dropna(subset=[code_col, name_col])\n",
    "          .groupby(code_col)[name_col]\n",
    "          .agg(lambda s: s.mode().iat[0])  # hay al menos 1 porque filtramos name_col no nulo\n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "    # 3) Rellenar NULOS en name_col usando el mapping (IN-PLACE)\n",
    "    mask = df[name_col].isna()\n",
    "    df.loc[mask, name_col] = df.loc[mask, code_col].map(mapping)\n",
    "\n",
    "    checks = {\n",
    "        \"cada_nombre_un_codigo\": len(nombres_conflictivos) == 0,\n",
    "        \"cada_codigo_un_nombre\": len(codigos_conflictivos) == 0,\n",
    "        \"nombres_conflictivos\": nombres_conflictivos,\n",
    "        \"codigos_conflictivos\": codigos_conflictivos,\n",
    "    }\n",
    "    return mapping, checks  # se modificó df IN-PLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3cbcef-ef2a-4f06-829d-722dead4861e",
   "metadata": {},
   "source": [
    "Pipeline de Normalización "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d773cb-83ff-4968-8aa3-b85308f61808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_accents(s: str) -> str:\n",
    "    # elimina tildes/acentos\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
    "\n",
    "def _normalize_token(s: str) -> str:\n",
    "    # a minúsculas, sin acentos, solo [a-z0-9_], sin guiones bajos repetidos\n",
    "    s = _strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)   # reemplaza todo lo no permitido por \"_\"\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\") # compacta y recorta \"_\"\n",
    "    return s or \"col\"                    # evita vacío\n",
    "\n",
    "def normalize_column_names(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza nombres de columnas: minúsculas, sin acentos, [a-z0-9_].\n",
    "    Resuelve colisiones añadiendo sufijos _2, _3, ...\n",
    "    Imprime la lista final de nombres si verbose=True.\n",
    "    Devuelve el DataFrame con columnas renombradas.\n",
    "    \"\"\"\n",
    "    original = list(df.columns)\n",
    "    normalized = []\n",
    "    used = {}\n",
    "\n",
    "    for col in original:\n",
    "        base = _normalize_token(str(col))\n",
    "        # asegura unicidad\n",
    "        name = base\n",
    "        k = 2\n",
    "        while name in used:\n",
    "            name = f\"{base}_{k}\"\n",
    "            k += 1\n",
    "        used[name] = True\n",
    "        normalized.append(name)\n",
    "\n",
    "    df = df.rename(columns=dict(zip(original, normalized)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Columnas normalizadas ({len(df.columns)}):\")\n",
    "        print(\"  \" + \", \".join(df.columns))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746ae6f-3432-43ec-b7ce-f716ce79195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_etiqueta_por_id(\n",
    "    df: pd.DataFrame,\n",
    "    catalogo: pd.DataFrame,\n",
    "    *,\n",
    "    col_codigo: str,       # ej: \"region_id\"\n",
    "    col_etiqueta: str,     # ej: \"region\"\n",
    "    cat_codigo: str,       # ej: \"Código Región\"\n",
    "    cat_etiqueta: str,     # ej: \"Nombre Región\"\n",
    "    solo_si_vacia: bool = False,  # True: solo rellena vacíos; False: sobrescribe donde haya ID mapeable\n",
    "    imprimir: bool = True,\n",
    "    max_items: int = 5\n",
    "):\n",
    "    # --- validaciones de columnas\n",
    "    if col_codigo not in df.columns or col_etiqueta not in df.columns:\n",
    "        raise KeyError(f\"Faltan columnas en df: {col_codigo!r} o {col_etiqueta!r}\")\n",
    "    if cat_codigo not in catalogo.columns or cat_etiqueta not in catalogo.columns:\n",
    "        raise KeyError(f\"Faltan columnas en catálogo: {cat_codigo!r} o {cat_etiqueta!r}\")\n",
    "\n",
    "    # --- preparar catálogo (eliminar duplicados exactos y NAs)\n",
    "    cat = (catalogo[[cat_codigo, cat_etiqueta]]\n",
    "           .dropna(subset=[cat_codigo, cat_etiqueta])\n",
    "           .drop_duplicates())\n",
    "\n",
    "    # --- construir mapping simple (asumiendo 1↔1 limpio)\n",
    "    # si quisieras forzar 1↔1, agrega una validación antes\n",
    "    mapping_id2name = cat.set_index(cat_codigo)[cat_etiqueta].to_dict()\n",
    "\n",
    "    # --- métricas antes\n",
    "    etq = df[col_etiqueta].astype(\"string\")\n",
    "    vacio_antes = (etq.isna() | etq.str.strip().eq(\"\"))\n",
    "    n_vacios_antes = int(vacio_antes.sum())\n",
    "\n",
    "    # --- reemplazo por ID\n",
    "    destino = df[col_codigo].map(mapping_id2name)\n",
    "    mapeable = destino.notna()\n",
    "    if solo_si_vacia:\n",
    "        objetivo = mapeable & (etq.isna() | etq.str.strip().eq(\"\"))\n",
    "    else:\n",
    "        objetivo = mapeable\n",
    "\n",
    "    n_asignadas = int(objetivo.sum())\n",
    "    if n_asignadas > 0:\n",
    "        # --------- FIX AQUÍ: usar 'objetivo' (no 'objeto') ---------\n",
    "        df.loc[objetivo, col_etiqueta] = destino[objetivo]\n",
    "\n",
    "    # --- métricas después\n",
    "    etq2 = df[col_etiqueta].astype(\"string\")\n",
    "    vacio_despues_mask = (etq2.isna() | etq2.str.strip().eq(\"\"))\n",
    "    n_vacios_despues = int(vacio_despues_mask.sum())\n",
    "\n",
    "    ejemplo_vacio = None\n",
    "    if n_vacios_despues > 0:\n",
    "        ix = vacio_despues_mask[vacio_despues_mask].index[0]\n",
    "        ejemplo_vacio = {\n",
    "            \"index\": int(ix) if isinstance(ix, (int,)) else str(ix),\n",
    "            col_codigo: df.loc[ix, col_codigo],\n",
    "            col_etiqueta: df.loc[ix, col_etiqueta]\n",
    "        }\n",
    "        for cand in [\"idaccident\", \"id\", \"id_accidente\"]:\n",
    "            if cand in df.columns:\n",
    "                ejemplo_vacio[cand] = df.loc[ix, cand]\n",
    "                break\n",
    "\n",
    "    checks = {\n",
    "        \"n_asignadas_por_codigo\": n_asignadas,\n",
    "        \"n_etiqueta_vacia_antes\": n_vacios_antes,\n",
    "        \"n_etiqueta_vacia_despues\": n_vacios_despues,\n",
    "        \"ejemplo_etiqueta_vacia\": ejemplo_vacio\n",
    "    }\n",
    "\n",
    "    if imprimir:\n",
    "        print(f\"[reemplazar_etiqueta_por_id] {col_codigo} -> {col_etiqueta} usando {cat_codigo} -> {cat_etiqueta}\")\n",
    "        print(f\" - filas con reemplazo por código: {checks['n_asignadas_por_codigo']}\")\n",
    "        print(f\" - '{col_etiqueta}' vacías antes:  {checks['n_etiqueta_vacia_antes']}\")\n",
    "        print(f\" - '{col_etiqueta}' vacías después:{checks['n_etiqueta_vacia_despues']}\")\n",
    "        if ejemplo_vacio:\n",
    "            print(f\"   * Ejemplo que sigue vacío: {ejemplo_vacio}\")\n",
    "\n",
    "    return mapping_id2name, checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d183c-0a68-4b08-9b9e-8e8052db7397",
   "metadata": {},
   "source": [
    "Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a641955-0c1b-42dc-9955-57bc02199b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_num(df, cols, dtype=\"Int64\"):\n",
    "    exist = [c for c in cols if c in df.columns]\n",
    "    if not exist:\n",
    "        return pd.Series(pd.NA, index=df.index, dtype=dtype)\n",
    "    tmp = pd.DataFrame({c: pd.to_numeric(df[c], errors=\"coerce\") for c in exist})\n",
    "    out = tmp.bfill(axis=1).iloc[:, 0]\n",
    "    conflict_mask = tmp.nunique(axis=1, dropna=True) > 1\n",
    "    return out.mask(conflict_mask, pd.NA).astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86914b7-22cd-4ca9-a20c-30d057ba5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_str(df, cols):\n",
    "    exist = [c for c in cols if c in df.columns]\n",
    "    if not exist:\n",
    "        return pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "    block = df[exist].astype(\"string\").apply(lambda s: s.str.strip())\n",
    "    out = block.bfill(axis=1).iloc[:, 0]\n",
    "    conflict_mask = block.nunique(axis=1, dropna=True) > 1\n",
    "    return out.mask(conflict_mask, pd.NA).astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b32150-0000-413c-a299-053ae1a35959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_datetime(df, cols):\n",
    "    \"\"\"\n",
    "    Primer datetime no nulo por fila entre varias columnas.\n",
    "    Marca NA si hay conflicto (más de un valor distinto no nulo).\n",
    "    \"\"\"\n",
    "    exist = [c for c in cols if c in df.columns]\n",
    "    if not exist:\n",
    "        return pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    # Parsear cada candidata a datetime64[ns]\n",
    "    parsed = {c: pd.to_datetime(df[c], errors=\"coerce\") for c in exist}\n",
    "    tmp = pd.DataFrame(parsed, index=df.index)\n",
    "\n",
    "    # Primer no nulo por fila (bfill horizontal y tomar la 1ª col)\n",
    "    out = tmp.bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    # Conflicto: más de un datetime distinto no nulo en la fila\n",
    "    conflict_mask = tmp.nunique(axis=1, dropna=True) > 1\n",
    "    return out.mask(conflict_mask, pd.NaT)\n",
    "\n",
    "\n",
    "def build_unique_map_str_multi(df, src_cols, tgt_col):\n",
    "    \"\"\"\n",
    "    Aprende dict src->tgt (string->string) usando *todas* las columnas src disponibles.\n",
    "    Solo acepta claves cuyo mapeo sea unívoco (1->1) considerando el conjunto completo.\n",
    "    \"\"\"\n",
    "    if not src_cols or tgt_col is None:\n",
    "        return {}\n",
    "    if tgt_col not in df.columns:\n",
    "        return {}\n",
    "\n",
    "    # Apilar pares (src, tgt) desde todas las columnas de detalle\n",
    "    frames = []\n",
    "    for c in src_cols:\n",
    "        if c in df.columns:\n",
    "            frames.append(pd.DataFrame({\"src\": df[c], \"tgt\": df[tgt_col]}))\n",
    "    if not frames:\n",
    "        return {}\n",
    "\n",
    "    sub = pd.concat(frames, axis=0, ignore_index=True).dropna()\n",
    "    if sub.empty:\n",
    "        return {}\n",
    "\n",
    "    # Normalizar para comparar (mantén lógica de tu _norm_txt)\n",
    "    sub = pd.DataFrame({\n",
    "        \"src\": _norm_txt(sub[\"src\"]),\n",
    "        \"tgt\": _norm_txt(sub[\"tgt\"]),\n",
    "    }).dropna()\n",
    "\n",
    "    # Quedarnos solo con src que tengan un único tgt en TODO el conjunto\n",
    "    counts = sub.groupby(\"src\")[\"tgt\"].nunique()\n",
    "    valid_src = counts[counts == 1].index\n",
    "    mapping = (sub[sub[\"src\"].isin(valid_src)]\n",
    "               .drop_duplicates(\"src\")\n",
    "               .set_index(\"src\")[\"tgt\"]\n",
    "               .to_dict())\n",
    "    return mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339664b1-712c-4cda-82e6-7efc9fc9f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unique_map_num_to_str_multi(df, src_num_cols, tgt_col):\n",
    "    \"\"\"\n",
    "    Aprende dict codigo(int)->tgt(str) combinando múltiples columnas numéricas.\n",
    "    Solo mapeos 1->1 globales.\n",
    "    \"\"\"\n",
    "    if not src_num_cols or tgt_col is None:\n",
    "        return {}\n",
    "    if tgt_col not in df.columns:\n",
    "        return {}\n",
    "\n",
    "    frames = []\n",
    "    for c in src_num_cols:\n",
    "        if c in df.columns:\n",
    "            frames.append(pd.DataFrame({\"src\": pd.to_numeric(df[c], errors=\"coerce\"),\n",
    "                                        \"tgt\": df[tgt_col]}))\n",
    "    if not frames:\n",
    "        return {}\n",
    "\n",
    "    sub = pd.concat(frames, axis=0, ignore_index=True).dropna()\n",
    "    if sub.empty:\n",
    "        return {}\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        \"src\": pd.to_numeric(sub[\"src\"], errors=\"coerce\").astype(\"Int64\"),\n",
    "        \"tgt\": _norm_txt(sub[\"tgt\"]),\n",
    "    }).dropna()\n",
    "\n",
    "    counts = sub.groupby(\"src\")[\"tgt\"].nunique()\n",
    "    valid_src = counts[counts == 1].index\n",
    "    mapping = (sub[sub[\"src\"].isin(valid_src)]\n",
    "               .drop_duplicates(\"src\")\n",
    "               .set_index(\"src\")[\"tgt\"]\n",
    "               .to_dict())\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7e0be-ef37-490e-a40c-32dd96d74c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_txt(s: pd.Series):\n",
    "    return (s.astype(\"string\")\n",
    "              .str.normalize(\"NFKD\")  # maneja tildes\n",
    "              .str.encode(\"ascii\", errors=\"ignore\").str.decode(\"ascii\")\n",
    "              .str.upper()\n",
    "              .str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ef9b9-bba3-4d46-9c0c-0c7165f3be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristica_tipo_grupo(txt):\n",
    "    \"\"\"Último recurso: inferir grupo (7 categorías) a partir de palabras clave.\"\"\"\n",
    "    if pd.isna(txt):\n",
    "        return pd.NA\n",
    "    t = str(txt).upper()\n",
    "    if \"CHOQUE\" in t:     return \"CHOQUE\"\n",
    "    if \"COLISION\" in t:   return \"COLISION\"\n",
    "    if \"ATROPELLO\" in t:  return \"ATROPELLO\"\n",
    "    if \"CAIDA\" in t:      return \"CAIDA\"\n",
    "    if \"VOLCADURA\" in t:  return \"VOLCADURA\"\n",
    "    if \"INCENDIO\" in t:   return \"INCENDIO\"\n",
    "    return \"OTRO TIPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c5c5f-8bcf-4417-855d-838355d6ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing(name, df):\n",
    "    n = len(df)\n",
    "    miss = df.isna().sum()\n",
    "    miss = miss[miss > 0].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{name}  |  filas={n}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if miss.empty:\n",
    "        print(\"✓ Sin datos faltantes (todas las columnas completas).\")\n",
    "    else:\n",
    "        print(\"Columnas con faltantes:\")\n",
    "        for col, cnt in miss.items():\n",
    "            pct = (cnt / n * 100) if n else float(\"nan\")\n",
    "            print(f\"  - {col}: {cnt} ({pct:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
