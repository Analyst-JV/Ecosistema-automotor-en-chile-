{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1d80b-9d3a-4584-a4c4-ecfe1d347a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from collections.abc import Mapping\n",
    "from typing import Dict, Optional\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a90db9-e2a3-4cf3-af9c-38d666e03ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARPETA_1 = \"Permisos\"\n",
    "CARPETA_2 = \"Licencias\"\n",
    "NOMBRE_TABLA_ESPECIFICA = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34d2a7-5150-4f2b-b13d-74398545fd53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "for carpeta in [CARPETA_1, CARPETA_2]:\n",
    "    for db in lista_bases(carpeta):\n",
    "        try:\n",
    "            dfs.update(leer_access(db, NOMBRE_TABLA_ESPECIFICA))\n",
    "            print(f\"OK: {db}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR leyendo {db}: {e}\")\n",
    "\n",
    "print(\"\\nResumen de DataFrames cargados:\")\n",
    "for k, df in dfs.items():\n",
    "    print(f\"  - {k}: {df.shape[0]} filas x {df.shape[1]} cols\")\n",
    "\n",
    "\n",
    "CREAR_VARIABLES = False  \n",
    "if CREAR_VARIABLES:\n",
    "    for k, df in dfs.items():\n",
    "        globals()[k] = df\n",
    "    print(f\"\\nSe crearon {len(dfs)} variables en el entorno con los nombres de arriba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51829d9-ae9f-4951-9e51-4c06b904cd90",
   "metadata": {},
   "source": [
    "Eliminacion de las bases de datos que no necesitamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199acef8-e48f-4a5d-8821-6455281d378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenedor = globals().get(\"dfs\", None)  \n",
    "eliminar_df_objetivo(contenedor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653d401-cbe4-49e3-ae10-6f9102f6fe74",
   "metadata": {},
   "source": [
    "Normalizacion de el nombre de las columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aebf46-63fd-4655-84f5-511d929fa577",
   "metadata": {},
   "outputs": [],
   "source": [
    "estandarizar_columnas_inplace(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023e2ff-05fd-43d3-9c3f-6df37baf470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in dfs.items():\n",
    "    print(f\"\\n>>> {k}\")\n",
    "    display(df.head(3))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff5505-657a-4c33-a68a-24504920e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComparaciÃ³n \n",
    "comparar_columnas_y_mostrar(dfs, usar_normalizacion=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb6d84-b812-4d20-8587-6fda411be849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo para corregir la inconsistencia de 'cod_sex'\n",
    "map_correccion_sexo = {'cod_sex': 'cod_sexo'}\n",
    "\n",
    "# Aplicamos la correcciÃ³n solo a los DataFrames de tipo 'licencia'\n",
    "renombrar_columnas_especificas(dfs, map_correccion_sexo, tipo_df=\"licencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e1467-22b4-4580-8d15-b0ac37616377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo para corregir la inconsistencia en vehiculo\n",
    "map_correccion = {'bencina': 'bencinero' , 'ano': 'aÃ±o'}\n",
    "\n",
    "renombrar_columnas_especificas(dfs,map_correccion, tipo_df='vehiculo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39293e-30f3-4b8d-af61-adfd498076c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analizar_diccionarios(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c466ea9-0288-49bb-83f1-0a85ad191a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "agregar_columna_aÃ±o(dfs)\n",
    "print(\"\\nâœ… PASO 3: AdiciÃ³n de la columna 'aÃ±o' completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720f241-cd71-48d6-8a25-6284d2807436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 4: Aplicar los diccionarios para crear las columnas de etiquetas\n",
    "aplicar_diccionarios_a_datos(dfs)\n",
    "print(\"\\nâœ… PASO 4: CreaciÃ³n de columnas de etiquetas completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207f47b-ea7b-4f1b-a87c-41abe1f43630",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesar_dataframes_vehiculos(dfs)\n",
    "print(\"\\nâœ… PASO 4: TransformaciÃ³n de datos de vehÃ­culos completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bd2a5-2a41-4167-b23f-6e0d1b626743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_consolidados = pipeline_final_consolidacion_y_limpieza(dfs)\n",
    "\n",
    "licencias = dataframes_consolidados.get('licencias')\n",
    "vehiculos = dataframes_consolidados.get('vehiculos')\n",
    "\n",
    "# VerificaciÃ³n final\n",
    "if licencias is not None:\n",
    "    print(\"\\n--- Resumen DataFrame 'licencias' ---\")\n",
    "    licencias.info()\n",
    "    display(licencias.head())\n",
    "\n",
    "if vehiculos is not None:\n",
    "    print(\"\\n--- Resumen DataFrame 'vehiculos' ---\")\n",
    "    vehiculos.info()\n",
    "    display(vehiculos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737c737-e131-4047-9a7b-57e86420e77e",
   "metadata": {},
   "source": [
    "Exportacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17875f63-2ef7-4bc6-a0d6-43e9f99db422",
   "metadata": {},
   "outputs": [],
   "source": [
    "licencias.to_csv(\"licencias.csv\", sep=';', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e43d3-d058-4dac-baf2-e3ecef0618d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiculos.to_csv(\"vehiculos.csv\", sep=';', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2f021-dac7-47fe-92b2-9c85e587c86e",
   "metadata": {},
   "source": [
    "Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720670d4-be7f-477e-b2a6-ee156b6fd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generales ===\n",
    "def normaliza_nombre(s: str) -> str:\n",
    "    \"\"\"Genera un nombre vÃ¡lido para variable/clave a partir de string de archivo o tabla.\"\"\"\n",
    "    s = os.path.splitext(os.path.basename(s))[0]  # quita ruta y extensiÃ³n si viniera\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    s = re.sub(r\"[^\\w_]\", \"\", s, flags=re.ASCII)\n",
    "    # evita que empiece por dÃ­gito\n",
    "    if re.match(r\"^\\d\", s):\n",
    "        s = f\"n_{s}\"\n",
    "    return s\n",
    "\n",
    "def obtener_tablas(cursor) -> list:\n",
    "    \"\"\"Devuelve lista de tablas 'reales' (excluye system tables y vistas si aplica).\"\"\"\n",
    "    tablas = []\n",
    "    for row in cursor.tables(tableType='TABLE'):\n",
    "        # row.table_name, row.table_type, etc.\n",
    "        # Filtra tablas del sistema si es necesario:\n",
    "        if not row.table_name.startswith(\"MSys\"):  # tÃ­picas del sistema Access\n",
    "            tablas.append(row.table_name)\n",
    "    return tablas\n",
    "\n",
    "def leer_access(db_path: str, tabla_objetivo: str | None = None) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Lee una base Access con el driver 'Microsoft Access Driver (*.mdb, *.accdb)'.\n",
    "    Devuelve dict {clave: DataFrame}, donde clave = \"<Archivo>__<Tabla>\".\n",
    "    \"\"\"\n",
    "    conn_str = (\n",
    "        r\"Driver={Microsoft Access Driver (*.mdb, *.accdb)};\"\n",
    "        rf\"DBQ={db_path};\"\n",
    "    )\n",
    "    dfs_local = {}\n",
    "    with pyodbc.connect(conn_str, autocommit=True) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        if tabla_objetivo is None:\n",
    "            tablas = obtener_tablas(cursor)\n",
    "        else:\n",
    "            tablas = [tabla_objetivo]\n",
    "\n",
    "        base_name = normaliza_nombre(db_path)\n",
    "        for t in tablas:\n",
    "            # Lee toda la tabla\n",
    "            query = f\"SELECT * FROM [{t}]\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            # Clave combinando nombre de archivo + nombre de tabla\n",
    "            t_norm = normaliza_nombre(t)\n",
    "            clave = f\"{base_name}__{t_norm}\"\n",
    "            dfs_local[clave] = df\n",
    "\n",
    "    return dfs_local\n",
    "\n",
    "def lista_bases(carpeta: str) -> list[str]:\n",
    "    \"\"\"Encuentra .accdb y .mdb en la carpeta dada (no recursivo; usa ** para recursivo).\"\"\"\n",
    "    patrones = [os.path.join(carpeta, \"*.accdb\"), os.path.join(carpeta, \"*.mdb\")]\n",
    "    archivos = []\n",
    "    for p in patrones:\n",
    "        archivos.extend(glob.glob(p))\n",
    "    return archivos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65954f-8c11-4e20-8eb8-73aa7bd95f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_bases(carpeta: str) -> list[str]:\n",
    "    \"\"\"Encuentra .accdb y .mdb en la carpeta dada (no recursivo; usa ** para recursivo).\"\"\"\n",
    "    patrones = [os.path.join(carpeta, \"*.accdb\"), os.path.join(carpeta, \"*.mdb\")]\n",
    "    archivos = []\n",
    "    for p in patrones:\n",
    "        archivos.extend(glob.glob(p))\n",
    "    return archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0b809-041e-477d-ba99-6eecf39be269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Utilidades de normalizaciÃ³n =====================\n",
    "\n",
    "def quitar_acentos(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def split_camel(s: str) -> str:\n",
    "    # Inserta espacios antes de mayÃºsculas para separar CamelCase\n",
    "    return re.sub(r\"(?<=[a-z0-9])([A-Z])\", r\" \\1\", s)\n",
    "\n",
    "def normaliza_para_busqueda(nombre: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Devuelve (norm_espacios, norm_junto) para bÃºsquedas robustas.\n",
    "    - norm_espacios: minus, sin acentos, separadores unificados y CamelCase separado.\n",
    "    - norm_junto: como lo anterior pero sin espacios (para casos tipo VehiculosSinMotor).\n",
    "    \"\"\"\n",
    "    s = str(nombre)\n",
    "    s = quitar_acentos(s)\n",
    "    s = split_camel(s)\n",
    "    s = s.lower()\n",
    "    # unifica separadores a espacio\n",
    "    s = re.sub(r\"[_\\-./]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    junto = s.replace(\" \", \"\")\n",
    "    return s, junto\n",
    "\n",
    "# ===================== Coincidencia con \"vehÃ­culos sin motor\" =====================\n",
    "\n",
    "# Regex tolerante a la falta de la \"i\" en \"vehÃ­culos\": \"veh[i]?culos\"\n",
    "PATRON_REGEX = re.compile(r\"\\bveh[i]?culos\\s+sin\\s+motor\\b\")\n",
    "\n",
    "def es_objetivo(nombre: str) -> bool:\n",
    "    norm, junto = normaliza_para_busqueda(nombre)\n",
    "    if PATRON_REGEX.search(norm):\n",
    "        return True\n",
    "    # match por concatenado, con y sin la \"i\"\n",
    "    if \"vehiculossinmotor\" in junto or \"vehculossinmotor\" in junto:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ===================== EliminaciÃ³n e impresiÃ³n =====================\n",
    "\n",
    "def eliminar_df_objetivo(contenedor_posible):\n",
    "    eliminados = []\n",
    "    conservados = []\n",
    "\n",
    "    if isinstance(contenedor_posible, Mapping):\n",
    "        # Modo dict (p.ej. dfs)\n",
    "        claves = list(contenedor_posible.keys())\n",
    "        for k in claves:\n",
    "            v = contenedor_posible.get(k, None)\n",
    "            if isinstance(v, pd.DataFrame) and es_objetivo(k):\n",
    "                eliminados.append(k)\n",
    "                del contenedor_posible[k]\n",
    "            elif isinstance(v, pd.DataFrame):\n",
    "                conservados.append(k)\n",
    "        modo = \"diccionario 'dfs'\"\n",
    "    else:\n",
    "        # Modo variables sueltas en globals()\n",
    "        vars_df = [k for k, v in globals().items() if isinstance(v, pd.DataFrame) and not k.startswith(\"_\")]\n",
    "        for k in vars_df:\n",
    "            if es_objetivo(k):\n",
    "                eliminados.append(k)\n",
    "                del globals()[k]\n",
    "            else:\n",
    "                conservados.append(k)\n",
    "        modo = \"variables globales\"\n",
    "\n",
    "    eliminados.sort(key=str.lower)\n",
    "    conservados.sort(key=str.lower)\n",
    "\n",
    "    print(f\"ðŸ”Ž Modo: {modo}\")\n",
    "    print(\"ðŸ§¹ Criterio: nombres que contengan 'veh(i)culos sin motor' (insensible a tildes/caso/separadores/CamelCase)\\n\")\n",
    "\n",
    "    print(f\"âœ… Eliminados ({len(eliminados)}):\")\n",
    "    if eliminados:\n",
    "        for n in eliminados:\n",
    "            print(f\"  - {n}\")\n",
    "    else:\n",
    "        print(\"  (ninguno)\")\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Conservados ({len(conservados)}):\")\n",
    "    if conservados:\n",
    "        for n in conservados:\n",
    "            print(f\"  - {n}\")\n",
    "    else:\n",
    "        print(\"  (ninguno)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5195165-f355-4391-8b17-fd1a96bd3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) UTILIDADES DE NORMALIZACIÃ“N\n",
    "def quitar_acentos(s: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Remueve diacrÃ­ticos y normaliza apÃ³strofes.\"\"\"\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.replace(\"â€™\", \"'\")\n",
    "\n",
    "def normaliza_nombre_col(col: str) -> str:\n",
    "    \"\"\"Normaliza nombres de columnas a snake_case sin tildes.\"\"\"\n",
    "    c = quitar_acentos(col).strip().lower()\n",
    "    for ch in (\" \", \"-\", \".\", \"/\"):\n",
    "        c = c.replace(ch, \"_\")\n",
    "    while \"__\" in c:\n",
    "        c = c.replace(\"__\", \"_\")\n",
    "    return c\n",
    "\n",
    "def normaliza_texto_serie(ser: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza texto en Series: quita tildes, trim, lower.\"\"\"\n",
    "    return (ser.astype(str)\n",
    "               .map(quitar_acentos)\n",
    "               .str.strip()\n",
    "               .str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f53dd-9f1c-4cec-830b-b3d92b8fcce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_columnas_por_df(dfs: dict, ordenar_columnas: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Genera un texto con viÃ±etas del tipo:\n",
    "      â€¢ nombre_tabla  (N columnas)\n",
    "          - col1\n",
    "          - col2\n",
    "    Retorna el string (y tambiÃ©n lo imprime).\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for nombre, df in dfs.items():\n",
    "        cols = list(df.columns)\n",
    "        if ordenar_columnas:\n",
    "            cols = sorted(cols)\n",
    "        lines.append(f\"  â€¢ {nombre}  ({len(cols)} columnas)\")\n",
    "        for c in cols:\n",
    "            lines.append(f\"      - {c}\")\n",
    "        lines.append(\"\")  # lÃ­nea en blanco entre tablas\n",
    "    resultado = \"\\n\".join(lines).rstrip()\n",
    "    print(resultado)\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f63291-b7d9-41f6-bdd0-0efcbf4e583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estandarizar_columnas_inplace(dfs: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Modifica EN SITIO los nombres de columnas de cada DataFrame en dfs,\n",
    "    usando normaliza_nombre_col. Soporta ColumnIndex y MultiIndex.\n",
    "    \"\"\"\n",
    "    for _, df in dfs.items():\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            # Normaliza cada nivel de cada tupla\n",
    "            new_cols = [\n",
    "                tuple(normaliza_nombre_col(x) for x in tpl)\n",
    "                for tpl in df.columns\n",
    "            ]\n",
    "            df.columns = pd.MultiIndex.from_tuples(new_cols, names=df.columns.names)\n",
    "        else:\n",
    "            # Normaliza columnas simples\n",
    "            df.rename(columns=lambda c: normaliza_nombre_col(c), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385a2e1-8a8e-4a0d-b776-ec486523a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_vehiculo(clave: str) -> bool:\n",
    "    parte = clave.split(\"__\", 1)[1] if \"__\" in clave else clave\n",
    "    t = parte.casefold().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    # Robusto a tildes/variantes: \"veh\", \"motor\", \"permiso(s)\", \"circulac(iÃ³n)\"\n",
    "    return ((\"veh\" in t and \"motor\" in t) or\n",
    "            (\"permiso\" in t and \"circulac\" in t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310ce51-d072-4eb2-ac0d-6f61b25bcae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_tabla(clave: str) -> str | None:\n",
    "    tabla = clave.split(\"__\", 1)[1] if \"__\" in clave else clave\n",
    "    t = tabla.casefold().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    if \"diccionario\" in t and \"variable\" in t:\n",
    "        return \"diccionario\"\n",
    "    if (\"lic\" in t or \"licencia\" in t) and \"conduc\" in t:\n",
    "        return \"licencia\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9225e6-d6de-4968-a221-647333711a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_key(s: str) -> str:\n",
    "    return s.casefold().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def _pick_ref(items, prefer_token: str):\n",
    "    \"\"\"Elige referencia por substring (robusto). Si no la encuentra, usa el primero.\"\"\"\n",
    "    token = _clean_key(prefer_token)\n",
    "    for it in items:\n",
    "        if token in _clean_key(it[\"name\"]):\n",
    "            return it\n",
    "    return items[0]\n",
    "\n",
    "def comparar_columnas_y_mostrar(\n",
    "    dfs: dict[str, pd.DataFrame],\n",
    "    usar_normalizacion: bool = False,\n",
    "    ref_licencia: str = \"2019\",\n",
    "    ref_diccionario: str = \"2019\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compara columnas dentro de cada grupo ('diccionario' y 'licencia'),\n",
    "    tomando como referencia la tabla que contenga `ref_diccionario` o `ref_licencia`\n",
    "    (por defecto, '2019') en el nombre de la clave.\n",
    "    \"\"\"\n",
    "    grupos = {\"diccionario\": [], \"licencia\": []}\n",
    "\n",
    "    for k, df in dfs.items():\n",
    "        tipo = clasificar_tabla(k)\n",
    "        if tipo in grupos:\n",
    "            cols_orig = list(map(str, df.columns))\n",
    "            if usar_normalizacion:\n",
    "                # Reutiliza tu normalizador; fallback simple si no existe\n",
    "                try:\n",
    "                    cols_cmp = set(norm_col_name(c) for c in cols_orig)\n",
    "                except NameError:\n",
    "                    try:\n",
    "                        cols_cmp = set(normaliza_nombre_col(c) for c in cols_orig)\n",
    "                    except NameError:\n",
    "                        cols_cmp = set(c.strip().lower().replace(\" \", \"_\") for c in cols_orig)\n",
    "            else:\n",
    "                cols_cmp = set(cols_orig)\n",
    "            grupos[tipo].append({\"name\": k, \"cols_orig\": cols_orig, \"cols_cmp\": cols_cmp})\n",
    "\n",
    "    for tipo, items in grupos.items():\n",
    "        print(f\"\\n================= {tipo.upper()} =================\")\n",
    "        if not items:\n",
    "            print(\"No se encontraron tablas de este tipo.\")\n",
    "            continue\n",
    "\n",
    "        # elegir referencia por grupo (prioriza 2019 por defecto)\n",
    "        prefer = ref_diccionario if tipo == \"diccionario\" else ref_licencia\n",
    "        ref = _pick_ref(items, prefer)\n",
    "        print(f\"Referencia: {ref['name']}  ({len(ref['cols_orig'])} columnas)\")\n",
    "\n",
    "        iguales, difs = [], []\n",
    "        for it in items:\n",
    "            faltantes = ref[\"cols_cmp\"] - it[\"cols_cmp\"]\n",
    "            extras    = it[\"cols_cmp\"] - ref[\"cols_cmp\"]\n",
    "            if not faltantes and not extras:\n",
    "                iguales.append(it[\"name\"])\n",
    "            else:\n",
    "                difs.append((it, faltantes, extras))\n",
    "\n",
    "        print(f\"\\nCoinciden con la referencia ({len(iguales)}):\")\n",
    "        for n in sorted(iguales):\n",
    "            print(f\"  - {n}\")\n",
    "\n",
    "        if not difs:\n",
    "            print(\"\\nâœ… Todas las tablas de este tipo tienen las mismas columnas\"\n",
    "                  + (\" (comparaciÃ³n normalizada).\" if usar_normalizacion else \".\"))\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nâš ï¸ Difieren de la referencia ({len(difs)}):\")\n",
    "        for it, falt, ext in difs:\n",
    "            print(f\"  - {it['name']}\")\n",
    "            if falt:\n",
    "                print(f\"      Â· Faltan vs ref: {sorted(falt)}\")\n",
    "            if ext:\n",
    "                print(f\"      Â· Sobran vs ref: {sorted(ext)}\")\n",
    "\n",
    "        print(\"\\nðŸ“‹ Listado de columnas por DataFrame (orden original):\")\n",
    "        for it in items:\n",
    "            print(f\"\\n  â€¢ {it['name']}  ({len(it['cols_orig'])} columnas)\")\n",
    "            for c in it[\"cols_orig\"]:\n",
    "                print(f\"      - {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b81194-2388-458c-89ad-789232ed5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_columnas_y_mostrar(\n",
    "    dfs: dict[str, pd.DataFrame],\n",
    "    usar_normalizacion: bool = False,\n",
    "    # Ahora puedes aÃ±adir tokens de referencia para cada tipo\n",
    "    ref_preferidos: dict[str, str] = {\"licencia\": \"2019\", \"diccionario\": \"2019\", \"vehiculo\": \"21\"}\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compara columnas dentro de cada grupo dinÃ¡micamente detectado.\n",
    "    \"\"\"\n",
    "    grupos = {}  \n",
    "\n",
    "    for k, df in dfs.items():\n",
    "       \n",
    "        tipo = clasificar_df_generico(k)\n",
    "        if tipo:\n",
    "            # Si el grupo (ej. 'vehiculo') no existe en el dict, lo crea\n",
    "            if tipo not in grupos:\n",
    "                grupos[tipo] = []\n",
    "\n",
    "            cols_orig = list(map(str, df.columns))\n",
    "            if usar_normalizacion:\n",
    "                cols_cmp = set(normaliza_nombre_col(c) for c in cols_orig)\n",
    "            else:\n",
    "                cols_cmp = set(cols_orig)\n",
    "            \n",
    "            grupos[tipo].append({\"name\": k, \"cols_orig\": cols_orig, \"cols_cmp\": cols_cmp})\n",
    "\n",
    "    for tipo, items in sorted(grupos.items()): # sorted() para un orden consistente\n",
    "        print(f\"\\n================= {tipo.upper()} ==================\")\n",
    "        if not items:\n",
    "            print(\"No se encontraron tablas de este tipo.\")\n",
    "            continue\n",
    "        \n",
    "        # Elige la referencia especÃ­fica para este tipo, o usa el primer item como fallback\n",
    "        prefer_token = ref_preferidos.get(tipo, \"\")\n",
    "        ref = _pick_ref(items, prefer_token)\n",
    "        print(f\"Referencia: {ref['name']}  ({len(ref['cols_orig'])} columnas)\")\n",
    "\n",
    "        iguales, difs = [], []\n",
    "        for it in items:\n",
    "            faltantes = ref[\"cols_cmp\"] - it[\"cols_cmp\"]\n",
    "            extras    = it[\"cols_cmp\"] - ref[\"cols_cmp\"]\n",
    "            if not faltantes and not extras:\n",
    "                iguales.append(it[\"name\"])\n",
    "            else:\n",
    "                difs.append((it, faltantes, extras))\n",
    "\n",
    "        print(f\"\\nCoinciden con la referencia ({len(iguales)}):\")\n",
    "        for n in sorted(iguales):\n",
    "            print(f\"  - {n}\")\n",
    "\n",
    "        if not difs:\n",
    "            print(f\"\\nâœ… Todas las tablas de este tipo tienen las mismas columnas\"\n",
    "                  + (\" (comparaciÃ³n normalizada).\" if usar_normalizacion else \".\"))\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nâš ï¸ Difieren de la referencia ({len(difs)}):\") \n",
    "        for it, falt, ext in difs:\n",
    "            print(f\"  - {it['name']}\")\n",
    "            if falt:\n",
    "                print(f\"      Â· Faltan vs ref: {sorted(falt)}\")\n",
    "            if ext:\n",
    "                print(f\"      Â· Sobran vs ref: {sorted(ext)}\")\n",
    "\n",
    "        print(\"\\nðŸ“‹ Listado de columnas por DataFrame (orden original):\")\n",
    "        for it in items:\n",
    "            print(f\"\\n  â€¢ {it['name']}  ({len(it['cols_orig'])} columnas)\")\n",
    "            for c in it[\"cols_orig\"]:\n",
    "                print(f\"      - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c2c7d-421c-404e-97e2-52fa06586281",
   "metadata": {},
   "source": [
    "Renombrador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5f728-cfb7-4fd3-b573-57109ca58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renombrar_columnas_especificas(dfs: dict[str, pd.DataFrame], mapeo_renombre: dict[str, str], tipo_df: str):\n",
    "    \"\"\"\n",
    "    Renombra columnas especÃ­ficas para un tipo de DataFrame dado, usando la clasificaciÃ³n genÃ©rica.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): El diccionario principal de DataFrames.\n",
    "        mapeo_renombre (dict): Un diccionario con {'nombre_antiguo': 'nombre_nuevo'}.\n",
    "        tipo_df (str): El tipo de tabla a procesar (ej. 'licencia', 'diccionario', 'vehiculo').\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Ejecutando renombrado especÃ­fico para DataFrames tipo '{tipo_df}'...\")\n",
    "    modificados = 0\n",
    "    for clave, df in dfs.items():\n",
    "        # Usamos la funciÃ³n de clasificaciÃ³n genÃ©rica que sÃ­ reconoce a los vehÃ­culos\n",
    "        if clasificar_df_generico(clave) == tipo_df:\n",
    "            columnas_a_renombrar = {k: v for k, v in mapeo_renombre.items() if k in df.columns}\n",
    "            \n",
    "            if columnas_a_renombrar:\n",
    "                df.rename(columns=columnas_a_renombrar, inplace=True)\n",
    "                print(f\"  - En '{clave}', se renombrÃ³: {columnas_a_renombrar}\")\n",
    "                modificados += 1\n",
    "    \n",
    "    if modificados == 0:\n",
    "        print(\"  âœ… No se encontraron columnas para renombrar con el mapeo proporcionado.\")\n",
    "    else:\n",
    "        print(f\"  âœ… Proceso finalizado. Se modificaron {modificados} DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4fe04-f6bf-4994-a3ff-a7ad0aa8202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_diccionarios(dfs: dict[str, pd.DataFrame], ejemplos_max: int = 5):\n",
    "    \"\"\"\n",
    "    Analiza y muestra un resumen de cada DataFrame de tipo 'diccionario'.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): El diccionario principal de DataFrames.\n",
    "        ejemplos_max (int): NÃºmero mÃ¡ximo de ejemplos de cÃ³digo->etiqueta a mostrar.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”Ž Analizando DataFrames de diccionarios...\")\n",
    "    \n",
    "    # Filtramos solo los diccionarios\n",
    "    dfs_diccionarios = {k: df for k, df in dfs.items() if clasificar_tabla(k) == \"diccionario\"}\n",
    "\n",
    "    if not dfs_diccionarios:\n",
    "        print(\"  No se encontraron DataFrames de tipo 'diccionario'.\")\n",
    "        return\n",
    "\n",
    "    for clave, df in dfs_diccionarios.items():\n",
    "        print(f\"\\nðŸ“˜ Diccionario: {clave}\")\n",
    "        \n",
    "        # Estandarizamos nombres de columnas para robustez\n",
    "        df.rename(columns=lambda c: normaliza_nombre_col(c), inplace=True)\n",
    "        \n",
    "        if 'variable' not in df.columns:\n",
    "            print(\"  âš ï¸ No se encontrÃ³ la columna 'variable'. Saltando este DataFrame.\")\n",
    "            continue\n",
    "            \n",
    "        variables = df['variable'].unique()\n",
    "        print(f\"ðŸ”¢ Total de variables: {len(variables)}\")\n",
    "        print(\"\\nLista de variables:\")\n",
    "        for var in variables:\n",
    "            print(f\"  - {var}\")\n",
    "\n",
    "        print(\"\\nResumen por variable (cantidad de cÃ³digos y ejemplos):\")\n",
    "        for var in variables:\n",
    "            sub_df = df[df['variable'] == var].dropna(subset=['valores', 'etiqueta'])\n",
    "            \n",
    "            # Crear ejemplos\n",
    "            ejemplos = [\n",
    "                f\"{row['valores']} â†’ {row['etiqueta']}\"\n",
    "                for _, row in sub_df.head(ejemplos_max).iterrows()\n",
    "            ]\n",
    "            ej_str = \";  \".join(ejemplos)\n",
    "            \n",
    "            # Usamos textwrap para un display limpio si la lÃ­nea es muy larga\n",
    "            linea_resumen = f\"â€¢ {var} â€” {len(sub_df)} cÃ³digos numÃ©ricos\"\n",
    "            print(linea_resumen)\n",
    "            \n",
    "            wrapped_ej = textwrap.fill(ej_str, width=100, subsequent_indent='     ')\n",
    "            print(f\"   ej.: {wrapped_ej}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b3209-9008-44e0-bacb-f1548a995344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_columna_aÃ±o(dfs: dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Agrega una columna 'aÃ±o' a los DataFrames de licencias, extraÃ­do de la clave.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”„ Agregando columna 'aÃ±o' a las bases de licencias...\")\n",
    "    agregados = []\n",
    "    sin_aÃ±o = []\n",
    "\n",
    "    for clave, df in dfs.items():\n",
    "        if not isinstance(df, pd.DataFrame) or not es_base_licencia(clave):\n",
    "            continue\n",
    "        \n",
    "        aÃ±o = extraer_aÃ±o(clave)\n",
    "        if aÃ±o is None:\n",
    "            sin_aÃ±o.append(clave)\n",
    "            continue\n",
    "\n",
    "        dfs[clave] = df.assign(aÃ±o=pd.Series([aÃ±o] * len(df), dtype=\"Int64\"))\n",
    "        agregados.append((clave, aÃ±o))\n",
    "\n",
    "    print(\"âœ… Columna 'anio' agregada/actualizada en:\")\n",
    "    for clave, aÃ±o in agregados:\n",
    "        print(f\"  - {clave}: aÃ±o={aÃ±o}\")\n",
    "\n",
    "    if sin_aÃ±o:\n",
    "        print(\"\\nâš ï¸ Bases sin aÃ±o detectable en el nombre:\")\n",
    "        for clave in sin_aÃ±o:\n",
    "            print(f\"  - {clave}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5aef0-aec3-4902-8d41-99fa7fdfe97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preparar_mapas_desde_diccionario(df_dic: pd.DataFrame) -> dict:\n",
    "    \"\"\"FunciÃ³n auxiliar para convertir un DF de diccionario a un mapa anidado.\"\"\"\n",
    "    mapas = {}\n",
    "    df_dic.rename(columns=lambda c: normaliza_nombre_col(c), inplace=True)\n",
    "    for var_nombre in df_dic['variable'].unique():\n",
    "        # Filtra, elimina duplicados y convierte a diccionario\n",
    "        mapa_var = df_dic[df_dic['variable'] == var_nombre]\\\n",
    "                        .drop_duplicates(subset=['valores'])\\\n",
    "                        .set_index('valores')['etiqueta']\\\n",
    "                        .to_dict()\n",
    "        mapas[var_nombre] = mapa_var\n",
    "    return mapas\n",
    "\n",
    "def aplicar_diccionarios_a_datos(dfs: dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Aplica los diccionarios a las bases de licencias para aÃ±adir columnas de etiquetas.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”„ Aplicando diccionarios para crear columnas de etiquetas...\")\n",
    "    \n",
    "    dfs_lic = {k: df for k, df in dfs.items() if es_base_licencia(k)}\n",
    "    dfs_dic = {k: df for k, df in dfs.items() if clasificar_tabla(k) == \"diccionario\"}\n",
    "\n",
    "    mapas_por_aÃ±o = {}\n",
    "    for clave_dic, df_dic in dfs_dic.items():\n",
    "        aÃ±o = extraer_aÃ±o(clave_dic)\n",
    "        if aÃ±o:\n",
    "            mapas_por_aÃ±o[aÃ±o] = _preparar_mapas_desde_diccionario(df_dic)\n",
    "\n",
    "    for clave_lic, df_lic in dfs_lic.items():\n",
    "        aÃ±o_lic = extraer_aÃ±o(clave_lic)\n",
    "        print(f\"\\nðŸ—‚ï¸ AÃ±o {aÃ±o_lic} â€” Base: {clave_lic}\")\n",
    "        \n",
    "        if not aÃ±o_lic or aÃ±o_lic not in mapas_por_aÃ±o:\n",
    "            print(\"  âš ï¸ No se encontrÃ³ un diccionario para este aÃ±o.\")\n",
    "            continue\n",
    "            \n",
    "        mapas_a_usar = mapas_por_aÃ±o[aÃ±o_lic]\n",
    "        clave_dic_usada = next(k for k in dfs_dic if str(aÃ±o_lic) in k)\n",
    "        print(f\"ðŸ“– Diccionario usado: {clave_dic_usada}\")\n",
    "\n",
    "        columnas_creadas = []\n",
    "        for col_codigo_raw, mapa in mapas_a_usar.items():\n",
    "            col_codigo = normaliza_nombre_col(col_codigo_raw)\n",
    "            \n",
    "            if col_codigo in df_lic.columns:\n",
    "                col_etiqueta = f\"{col_codigo}_eti\"\n",
    "                \n",
    "                # Forzamos la columna de datos al mismo tipo numÃ©rico que el mapa.\n",
    "                df_lic[col_codigo] = pd.to_numeric(df_lic[col_codigo], errors='coerce')\n",
    "\n",
    "                df_lic[col_etiqueta] = df_lic[col_codigo].map(mapa)\n",
    "                \n",
    "                mapeados = df_lic[col_etiqueta].notna().sum()\n",
    "                total = len(df_lic)\n",
    "                no_mapeados = total - mapeados\n",
    "                \n",
    "                if col_etiqueta not in columnas_creadas:\n",
    "                    columnas_creadas.append(col_etiqueta)\n",
    "\n",
    "                print(f\"  â€¢ {col_etiqueta}: etiquetas={mapeados}/{total}, cÃ³digos no mapeados={no_mapeados}\")\n",
    "\n",
    "        if columnas_creadas:\n",
    "            print(f\"âœ… Columnas de etiquetas creadas/actualizadas:\\n  - \" + \"\\n  - \".join(columnas_creadas))\n",
    "        else:\n",
    "            print(\"  â„¹ï¸ No se crearon columnas nuevas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a9702-91b4-4436-8cad-f394d583a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_base_licencia(clave: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determina si un DataFrame corresponde a una base de datos de licencias (y no un diccionario).\n",
    "    \"\"\"\n",
    "    k = clave.lower()\n",
    "    # Busca 'lic'/'licencia' Y 'conduc', pero EXCLUYE 'diccionario'\n",
    "    return ((\"lic\" in k or \"licencia\" in k) and \"conduc\" in k) and (\"diccionario\" not in k)\n",
    "\n",
    "def extraer_aÃ±o(clave: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Extrae el aÃ±o (ej. 2018, 2023) del nombre de la clave del DataFrame.\n",
    "    \"\"\"\n",
    "    # Busca un patrÃ³n de 4 dÃ­gitos que empiece con '20'\n",
    "    match = re.search(r\"(20\\d{2})\", clave)\n",
    "    return int(match.group(1)) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286236e-308f-4897-af1b-b8bd9d330a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_df_generico(clave: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Clasifica un DataFrame en un grupo ('diccionario', 'licencia', 'vehiculo') \n",
    "    basado en su clave.\n",
    "    \"\"\"\n",
    "    # Reutilizamos las funciones que ya tienes\n",
    "    if clasificar_tabla(clave) == \"diccionario\":\n",
    "        return \"diccionario\"\n",
    "    if clasificar_tabla(clave) == \"licencia\":\n",
    "        return \"licencia\"\n",
    "    if es_vehiculo(clave):\n",
    "        return \"vehiculo\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d4918-b50c-42f9-aa9f-e9be55af0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_dataframes_vehiculos(dfs: dict[str, pd.DataFrame], verificar: bool = True):\n",
    "    \"\"\"\n",
    "    Suma las columnas de tipo de combustible para crear 'cantidad_vehiculos',\n",
    "    verifica el total y elimina las columnas originales.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): El diccionario de DataFrames a modificar in-place.\n",
    "        verificar (bool): Si es True, comprueba que la suma de combustibles coincida\n",
    "                          con la suma de catalÃ­ticos antes de borrar columnas.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”„ Procesando DataFrames de VehÃ­culos...\")\n",
    "    \n",
    "    # Columnas de combustible (incluye variaciones como 'bencina' y 'bencinero')\n",
    "    cols_combustible_base = [\"bencina\", \"bencinero\", \"diesel\", \"gas\", \"electrico\", \"otro\"]\n",
    "    \n",
    "    for clave, df in dfs.items():\n",
    "        if not es_vehiculo(clave):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  -> Procesando '{clave}'...\")\n",
    "        \n",
    "        # 1. Identificar las columnas de combustible que existen en este DF especÃ­fico\n",
    "        cols_a_sumar = [col for col in cols_combustible_base if col in df.columns]\n",
    "        if not cols_a_sumar:\n",
    "            print(\"     - âš ï¸ No se encontraron columnas de combustible para sumar. Saltando.\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Crear la nueva columna 'cantidad_vehiculos'\n",
    "        df['cantidad_vehiculos'] = df[cols_a_sumar].sum(axis=1)\n",
    "        print(f\"     - âœ… Columna 'cantidad_vehiculos' creada.\")\n",
    "\n",
    "        # 3. Paso de VerificaciÃ³n\n",
    "        cols_verificacion = ['catalitico', 'nocatalitico']\n",
    "        if verificar and all(c in df.columns for c in cols_verificacion):\n",
    "            suma_verificacion = df[cols_verificacion].sum(axis=1)\n",
    "            \n",
    "            # Comparamos si las dos sumas son iguales\n",
    "            if df['cantidad_vehiculos'].equals(suma_verificacion):\n",
    "                print(\"     - âœ… VerificaciÃ³n exitosa: Suma de combustibles == Suma de catalÃ­ticos.\")\n",
    "                \n",
    "                # 4. Eliminar columnas originales solo si la verificaciÃ³n pasa\n",
    "                df.drop(columns=cols_a_sumar + cols_verificacion, inplace=True)\n",
    "                print(f\"     - âœ… Columnas eliminadas: {cols_a_sumar + cols_verificacion}\")\n",
    "            else:\n",
    "                print(\"     - âŒ ERROR DE VERIFICACIÃ“N: Las sumas no coinciden. No se eliminarÃ¡n las columnas.\")\n",
    "        else:\n",
    "            # Si no se puede verificar, simplemente se eliminan las de combustible\n",
    "            print(\"     - âš ï¸ No se pudo realizar la verificaciÃ³n. Eliminando solo columnas de combustible.\")\n",
    "            df.drop(columns=cols_a_sumar, inplace=True)\n",
    "            print(f\"     - âœ… Columnas eliminadas: {cols_a_sumar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f5af6-e6f3-4ff6-b104-6671be3292f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidar_dataframes_por_tipo(dfs: dict[str, pd.DataFrame], tipo: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Filtra los DataFrames de un tipo especÃ­fico y los concatena en uno solo.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): El diccionario completo de DataFrames.\n",
    "        tipo (str): El tipo a consolidar (ej. 'licencia', 'vehiculo').\n",
    "\n",
    "    Returns:\n",
    "        Un Ãºnico DataFrame consolidado o None si no se encontraron DFs de ese tipo.\n",
    "    \"\"\"\n",
    "    # 1. Filtrar los DataFrames que coinciden con el tipo deseado\n",
    "    dfs_a_consolidar = {k: df for k, df in dfs.items() if clasificar_df_generico(k) == tipo}\n",
    "    \n",
    "    if not dfs_a_consolidar:\n",
    "        print(f\"â„¹ï¸ No se encontraron DataFrames de tipo '{tipo}' para consolidar.\")\n",
    "        return None\n",
    "\n",
    "    print(f\" consolidating {len(dfs_a_consolidar)} DataFrames de tipo '{tipo}'...\")\n",
    "    \n",
    "    # Extraer la lista de DataFrames para concatenar\n",
    "    lista_dfs = list(dfs_a_consolidar.values())\n",
    "    \n",
    "    # 2. Concatenar todos los DataFrames en uno solo\n",
    "    df_consolidado = pd.concat(lista_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ… DataFrame '{tipo}' consolidado exitosamente.\")\n",
    "    print(f\"   -> Dimensiones finales: {df_consolidado.shape[0]} filas x {df_consolidado.shape[1]} columnas.\")\n",
    "    \n",
    "    return df_consolidado\n",
    "\n",
    "\n",
    "def pipeline_final_consolidacion_y_limpieza(dfs: dict[str, pd.DataFrame]) -> dict:\n",
    "    \"\"\"\n",
    "    Ejecuta la consolidaciÃ³n de licencias y vehÃ­culos, y limpia el diccionario original.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): El diccionario principal de DataFrames que serÃ¡ modificado.\n",
    "\n",
    "    Returns:\n",
    "        Un nuevo diccionario con los DataFrames consolidados ('licencias', 'vehiculos').\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸš€ INICIANDO PIPELINE FINAL: CONSOLIDACIÃ“N Y LIMPIEZA ðŸš€\")\n",
    "    \n",
    "    # 1. Consolidar Licencias\n",
    "    df_licencias = consolidar_dataframes_por_tipo(dfs, 'licencia')\n",
    "    \n",
    "    # 2. Consolidar VehÃ­culos\n",
    "    df_vehiculos = consolidar_dataframes_por_tipo(dfs, 'vehiculo')\n",
    "    \n",
    "    # 3. Limpieza del diccionario 'dfs' original\n",
    "    print(\"\\nðŸ§¹ Limpiando el diccionario 'dfs' original...\")\n",
    "    claves_a_eliminar = [k for k in dfs if clasificar_df_generico(k) in ['licencia', 'vehiculo', 'diccionario']]\n",
    "    \n",
    "    for clave in claves_a_eliminar:\n",
    "        del dfs[clave]\n",
    "        \n",
    "    print(f\"âœ… Se eliminaron {len(claves_a_eliminar)} DataFrames individuales y diccionarios.\")\n",
    "    if not dfs:\n",
    "        print(\"   -> El diccionario 'dfs' ahora estÃ¡ vacÃ­o.\")\n",
    "    else:\n",
    "        print(f\"   -> Quedan {len(dfs)} DataFrames sin clasificar en 'dfs'.\")\n",
    "\n",
    "    # 4. Crear un nuevo diccionario con los resultados finales\n",
    "    dataframes_finales = {}\n",
    "    if df_licencias is not None:\n",
    "        dataframes_finales['licencias'] = df_licencias\n",
    "    if df_vehiculos is not None:\n",
    "        dataframes_finales['vehiculos'] = df_vehiculos\n",
    "        \n",
    "    print(\"\\nðŸŽ‰ PIPELINE FINAL COMPLETADO ðŸŽ‰\")\n",
    "    return dataframes_finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59035110-1b94-47d4-b68f-a4692dfb1276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
